{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "import re\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'64bit'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.architecture()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mov_lines_file=open(\"/home/kavin/Silo/College Work/AI/DataSets/cornell_movie_dialogs_corpus/cornell movie-dialogs corpus/movie_lines.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mov_lines=mov_lines_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues=[]\n",
    "for mov_line in mov_lines:\n",
    "    matched=re.match('.* \\+\\+\\+\\$\\+\\+\\+ (.+)', mov_line)\n",
    "    dialogues.append(matched.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_dialogues=[]\n",
    "for line in dialogues:\n",
    "    tokenized_dialogues.append(filter(None,re.split(' |!|:|;|\\?|\"|\\-\\-|\\*|<.*?>|</.*?>', line.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_dialogues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dialogues=[]\n",
    "for line in tokenized_dialogues:\n",
    "    line_list=[]\n",
    "    for token in line:\n",
    "        try:\n",
    "            matched=re.match('([\\*|\\'|_|\\-|\\[]|)(.*)([\\*|\\'|_|\\-|\\]]|)', token)\n",
    "            line_list.append(matched.group(2))\n",
    "        except:\n",
    "            pass\n",
    "    cleaned_dialogues.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dialogues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_dialogues=[]\n",
    "for line in cleaned_dialogues:\n",
    "    all_dialogues+=line\n",
    "tokens=pd.Series(all_dialogues).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words={}\n",
    "index_to_words={}\n",
    "index=1\n",
    "for word in tokens:\n",
    "    bag_of_words[word]=index\n",
    "    index_to_words[index]=word\n",
    "    index+=1\n",
    "#n_chars = len(tokenized)\n",
    "n_vocab = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"gen_bag_of_words.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([bag_of_words.keys(), bag_of_words.values()])\n",
    "with open(\"gen_index_to_words.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows([index_to_words.keys(), index_to_words.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 284,\n",
       " 'fawn': 67833,\n",
       " 'petra,': 110346,\n",
       " 'considered.': 59177,\n",
       " 'body-guard.': 36841,\n",
       " 'considered,': 71798,\n",
       " 'gah': 31325,\n",
       " 'woods': 20038,\n",
       " 'sending.': 62178,\n",
       " 'spiders': 63796,\n",
       " 'hanging': 5748,\n",
       " 'woody': 63466,\n",
       " 'hastily': 66633,\n",
       " \"bringin'\": 27587,\n",
       " \"wainwright...'\": 93122,\n",
       " 'localized': 55707,\n",
       " 'spidery': 46515,\n",
       " 'tipsy.': 24289,\n",
       " 'tipsy,': 34235,\n",
       " 'know...right': 89884,\n",
       " 'chatter': 44782,\n",
       " 'course...': 23788,\n",
       " 'cake,': 13623,\n",
       " 'crossbar': 56804,\n",
       " 'full-blooded,': 87885,\n",
       " 'faggot,': 16401,\n",
       " 'undead.': 10507,\n",
       " 'undead,': 27007,\n",
       " 'goodnight.': 18394,\n",
       " 'more...logical': 88649,\n",
       " 'can\\t\\t\\t\\t': 107192,\n",
       " 'overcharge': 97045,\n",
       " 'wax.': 95288,\n",
       " \"wood'\": 62892,\n",
       " 'spider.': 53203,\n",
       " 'donnie...but': 88940,\n",
       " 'wood,': 27061,\n",
       " 'wood.': 31534,\n",
       " 'bogyman': 82768,\n",
       " 'impotent.': 52015,\n",
       " 'zulu.': 110622,\n",
       " \"hangin'\": 15399,\n",
       " 'steak...': 95783,\n",
       " 'bringing': 7396,\n",
       " 'stronger...': 55182,\n",
       " \"what'sit.\": 103580,\n",
       " 'n-nobody...': 107859,\n",
       " 'elevates': 95233,\n",
       " 'jackasses.': 108239,\n",
       " 'of...shit.': 12264,\n",
       " 'wooden': 14766,\n",
       " 'unfulfilled,': 67480,\n",
       " 'wednesday': 19107,\n",
       " 'hr': 12297,\n",
       " 'yes...james,': 63349,\n",
       " 'circuitry': 100966,\n",
       " 'crotch': 17585,\n",
       " 'which-though': 72089,\n",
       " 'showpony.': 97840,\n",
       " 'south-west': 87172,\n",
       " 'cane,': 108977,\n",
       " 'cane.': 51137,\n",
       " 'everything..': 17120,\n",
       " 'ex-catcher.': 72864,\n",
       " 'coordinate': 12796,\n",
       " 'sheepish.': 106637,\n",
       " \"tom's\": 10901,\n",
       " 'berserk.': 24059,\n",
       " 'berserk,': 65215,\n",
       " 'harkonnens....': 75893,\n",
       " '27,': 64935,\n",
       " '27.': 86579,\n",
       " \"on...how's\": 70731,\n",
       " '270': 28036,\n",
       " 'perforation.': 104034,\n",
       " \"scanner's\": 54824,\n",
       " 'gorman': 9195,\n",
       " 'consenting': 5697,\n",
       " 'errors': 47466,\n",
       " 'rhodes.': 74685,\n",
       " 'prisoners-of-war.': 48936,\n",
       " 'rhodes,': 74617,\n",
       " \"screamin'\": 44420,\n",
       " \"rhodes'\": 74714,\n",
       " 'designing': 30484,\n",
       " 'um...yes.': 109200,\n",
       " 'numeral': 60372,\n",
       " 'pawed': 22263,\n",
       " 'sunflowers.': 73849,\n",
       " 'kid,': 5374,\n",
       " \"e'\": 62437,\n",
       " 'kid.': 2518,\n",
       " 'internet...': 107714,\n",
       " 'night-cycle.': 100853,\n",
       " 'adcox...': 14344,\n",
       " 'disobeying': 55777,\n",
       " 'folks...': 47883,\n",
       " \"how've\": 20131,\n",
       " 'brainwashed': 40293,\n",
       " 'meteors.': 85418,\n",
       " 'canes': 85676,\n",
       " 'titles,': 19939,\n",
       " 'titles.': 63121,\n",
       " 'gasket,': 25539,\n",
       " 'affiliated': 48168,\n",
       " 'gasket.': 107550,\n",
       " \"what's,\": 52672,\n",
       " 'mcdonalds.': 36473,\n",
       " 'mcdonalds,': 65387,\n",
       " 'existentialist': 38269,\n",
       " 'kids': 2860,\n",
       " 'talby': 21864,\n",
       " 'anti-virus,': 91213,\n",
       " 'twelve-fifteen.': 81137,\n",
       " 'serge..': 7627,\n",
       " 'loafers.': 109550,\n",
       " 'controversy': 86598,\n",
       " 'kidd': 25654,\n",
       " 'targeting': 48943,\n",
       " 'neurologist': 61395,\n",
       " \"cavalier.'\": 96481,\n",
       " 'error,': 27297,\n",
       " 'error.': 32976,\n",
       " 'w-w-we': 93333,\n",
       " 'projection': 71718,\n",
       " 'ger...': 14207,\n",
       " 'stern': 46734,\n",
       " 'population.': 4134,\n",
       " 'northeast.': 45002,\n",
       " 'delorean.': 22878,\n",
       " 'dna': 5113,\n",
       " 'minors...': 92695,\n",
       " 'dnc': 59193,\n",
       " 'insecurity': 62665,\n",
       " 'naive,': 24715,\n",
       " 'ramstein': 19614,\n",
       " 'callsheet,': 101272,\n",
       " 'dusan.': 33363,\n",
       " 'bruce...': 31432,\n",
       " 'sermons': 66711,\n",
       " 'show,': 1341,\n",
       " 'tampering,': 60520,\n",
       " 'romanovs.': 32818,\n",
       " 'populations': 1653,\n",
       " 'nincom': 84888,\n",
       " 'yahoo': 29151,\n",
       " 'endurance.': 93952,\n",
       " 'freqs,': 83284,\n",
       " 'freqs.': 95327,\n",
       " 'pinched..': 91191,\n",
       " 'oxbridge.': 66479,\n",
       " 'endurance,': 99866,\n",
       " 'sermon.': 53112,\n",
       " 'intake': 55638,\n",
       " 'morally': 3263,\n",
       " 'distortion,': 54415,\n",
       " 'distortion.': 24971,\n",
       " \"ester'1l\": 92400,\n",
       " 'liability,': 31189,\n",
       " 'liability.': 19198,\n",
       " 'b.s.-ing': 79075,\n",
       " 'sidebar.': 102589,\n",
       " \"hudsucker's\": 84987,\n",
       " 'expressively': 81548,\n",
       " \"india's\": 80682,\n",
       " 'recondo': 64900,\n",
       " 'caporegime': 57199,\n",
       " 'wang': 14124,\n",
       " 'wand': 65419,\n",
       " 'pints': 70929,\n",
       " 'dinosaur.': 14452,\n",
       " 'dinosaur,': 87676,\n",
       " 'disparagement': 96751,\n",
       " 'titanium': 25466,\n",
       " 'kids...': 39913,\n",
       " 'pinta': 43044,\n",
       " 'want': 218,\n",
       " 'cyberdyne...': 103074,\n",
       " 'pinto': 52791,\n",
       " 'cocksucker': 32299,\n",
       " 'unmade,': 98689,\n",
       " 'no-how.': 92635,\n",
       " 'wracked': 82854,\n",
       " 'collins.': 10569,\n",
       " 'travel': 11582,\n",
       " 'cherlindrea,': 109937,\n",
       " 'glenn,': 59932,\n",
       " 'copious': 88257,\n",
       " 'glenn.': 21433,\n",
       " 'mulvaney...': 75456,\n",
       " 'minute...okay,': 72320,\n",
       " 'petesakes.': 46647,\n",
       " 'darryl,': 58785,\n",
       " 'possession...': 109743,\n",
       " '2015.': 17981,\n",
       " 'spider,': 107511,\n",
       " 'wudan.': 73933,\n",
       " '...say': 89175,\n",
       " 'goober': 81470,\n",
       " 'ex-fbi': 37722,\n",
       " 'assimilated': 52237,\n",
       " 'ear-hole,': 88234,\n",
       " 'dinosaurs': 37617,\n",
       " 'wrong': 1045,\n",
       " 'cuba...': 90588,\n",
       " 'wristlet': 72547,\n",
       " 'sentencing': 66568,\n",
       " 'glandular': 66446,\n",
       " 'syphilis.': 13124,\n",
       " 'recombination': 17831,\n",
       " 'airlock.': 25131,\n",
       " 'retired.': 13000,\n",
       " 'a-cup': 96120,\n",
       " \"bobo's\": 31298,\n",
       " 'bembry,': 42764,\n",
       " 'rmprmmh': 100249,\n",
       " 'snugly': 70022,\n",
       " 'calves...': 82531,\n",
       " 'effect.': 21346,\n",
       " 'effect,': 27009,\n",
       " 'voucher.': 38660,\n",
       " 'shiva,': 54093,\n",
       " 'shiva.': 35405,\n",
       " 'activating': 44269,\n",
       " 'na\\xefve': 102656,\n",
       " 'welcomes': 20480,\n",
       " 'fir': 39520,\n",
       " 'fis': 2964,\n",
       " 'gevs': 30232,\n",
       " 'fit': 1095,\n",
       " 'lifeline': 22957,\n",
       " 'screaming': 9744,\n",
       " 'fix': 3299,\n",
       " 'striking': 27445,\n",
       " 'sugarplum.': 53263,\n",
       " 'weightless.': 102764,\n",
       " 'fig': 16139,\n",
       " 'fie': 40743,\n",
       " 'discharge.': 57643,\n",
       " 'cullum,': 73399,\n",
       " 'cullum.': 73392,\n",
       " 'fin': 93617,\n",
       " 'welcome,': 13876,\n",
       " 'jacked-up': 81725,\n",
       " 'welcome.': 520,\n",
       " 'paterfamilias': 48406,\n",
       " 'songwriter': 29660,\n",
       " 'spot...': 72646,\n",
       " 'third-baseman': 97458,\n",
       " 'effects': 8755,\n",
       " \"egelhoffer's\": 33962,\n",
       " 'sixty-forty...': 46959,\n",
       " 'zoids.': 42167,\n",
       " 'bankrupt.': 43198,\n",
       " 'bankrupt,': 14397,\n",
       " 'grahzny,': 72533,\n",
       " 'u-huh.': 63847,\n",
       " 'whacking': 67045,\n",
       " 'bartok': 63634,\n",
       " 'seamstress': 14672,\n",
       " 'business...but': 81438,\n",
       " 'barton': 12360,\n",
       " '1919...i': 81417,\n",
       " \"fi'\": 82112,\n",
       " 'ingrid': 57203,\n",
       " 'restaurants,': 50934,\n",
       " 'restaurants.': 31308,\n",
       " 'cramming.': 69687,\n",
       " 'romulans': 54537,\n",
       " 'pumpkin,': 7868,\n",
       " \"episodes.'\": 61296,\n",
       " 'pumpkin.': 29305,\n",
       " 'parasites': 27034,\n",
       " 'mansons': 109745,\n",
       " 'job...': 4423,\n",
       " \"jenkins'\": 97497,\n",
       " 'nuys.': 28166,\n",
       " 'impossible': 10045,\n",
       " 'worn-out': 16357,\n",
       " 'jenkins.': 49896,\n",
       " 'up\\x97come': 37000,\n",
       " 'jenkins,': 107721,\n",
       " 'self.': 18265,\n",
       " 'self-': 22381,\n",
       " 'self,': 52526,\n",
       " 'reeves,': 61415,\n",
       " 'reeves.': 43362,\n",
       " 'indiscretion': 36207,\n",
       " 'simpler,': 67144,\n",
       " 'simpler.': 9997,\n",
       " 'eighty-seven-point-': 55308,\n",
       " 'surplices,': 77892,\n",
       " 'adapt': 4713,\n",
       " 'pure...': 74148,\n",
       " 'moving...well,': 62428,\n",
       " 'buttercup,': 100339,\n",
       " 'abbott': 19604,\n",
       " 'hades.': 99481,\n",
       " 'hades,': 41393,\n",
       " 'grapefruit.': 52964,\n",
       " 'parasite,': 84450,\n",
       " 'parasite.': 20584,\n",
       " 'pumpkins': 21001,\n",
       " 'romulan,': 54922,\n",
       " 'victorious.': 105316,\n",
       " 'romulan.': 54544,\n",
       " 'kawasaki,': 82637,\n",
       " 'estimate': 11172,\n",
       " 'kincaid.': 70552,\n",
       " 'silent': 16510,\n",
       " 'married...lily': 89025,\n",
       " \"mortal's\": 85283,\n",
       " '$800,000': 85768,\n",
       " 'kiersey': 51658,\n",
       " \"grabthar's\": 27755,\n",
       " 'receptionist.': 95655,\n",
       " 'disturbed': 27292,\n",
       " 'guardians.': 43053,\n",
       " 'nasty.': 13610,\n",
       " 'nasty,': 70173,\n",
       " 'garcetti,': 97618,\n",
       " 'receptionist,': 39928,\n",
       " 'garcetti.': 97587,\n",
       " 'purpose': 1024,\n",
       " 'size': 3992,\n",
       " 'curiouser': 82174,\n",
       " 'buy...as': 76777,\n",
       " 'i...listen,': 60689,\n",
       " \"list...here's\": 81014,\n",
       " 'reward,': 37061,\n",
       " 'phraseology.': 83105,\n",
       " 'reward.': 35622,\n",
       " 'olds': 35075,\n",
       " 'shampoo,': 100110,\n",
       " 'forrester': 60609,\n",
       " 'frenchman...': 61878,\n",
       " 'noncolor,': 42131,\n",
       " 'needed': 4980,\n",
       " 'codswopple.': 85842,\n",
       " 'master': 6147,\n",
       " 'understan': 67550,\n",
       " \"feelin'\": 4488,\n",
       " 'embezzling,': 103723,\n",
       " 'genesis': 44321,\n",
       " 'embezzling.': 91813,\n",
       " 'fedens': 84252,\n",
       " 'berlioz': 104711,\n",
       " 'rewards': 24531,\n",
       " 'users...you': 107072,\n",
       " 'scrapes': 65818,\n",
       " '...pills,': 69157,\n",
       " 'mutilated': 10424,\n",
       " 'to...have': 71362,\n",
       " 'accords.': 84170,\n",
       " 'positively': 31138,\n",
       " 'sheek': 75723,\n",
       " 'renovate,': 100509,\n",
       " 'tiger...': 58498,\n",
       " 'understa-': 82479,\n",
       " 'addicted.': 77130,\n",
       " 'silky': 93568,\n",
       " 'feeling': 1555,\n",
       " 'old-': 21098,\n",
       " 'old,': 10213,\n",
       " 'confront...': 93238,\n",
       " 'old.': 3739,\n",
       " 'flapjacks.': 65719,\n",
       " \"bleeker's\": 39212,\n",
       " 'oars.': 57338,\n",
       " 'waconda': 30446,\n",
       " 'pecs': 97449,\n",
       " 'bogan': 100730,\n",
       " 'errand-boy.': 5521,\n",
       " 'well...flashlight': 102781,\n",
       " 'mordechai.': 104725,\n",
       " 'mubutu': 97172,\n",
       " 'affairs': 18669,\n",
       " 'chin.': 28719,\n",
       " 'wholesome': 15678,\n",
       " 'courier': 46067,\n",
       " 'hymen': 71177,\n",
       " 'fugitive.': 61701,\n",
       " 'paramedics': 53223,\n",
       " 'fugitive,': 83317,\n",
       " 'agincourt.': 105356,\n",
       " 'circling.': 64906,\n",
       " 'pee-pee': 34329,\n",
       " 'shifting.': 109577,\n",
       " 'masterpiece...': 108721,\n",
       " 'willing,': 9403,\n",
       " '$10,000': 103708,\n",
       " 'willing.': 19937,\n",
       " 'kremlin': 46679,\n",
       " 'shipments': 26957,\n",
       " 'planation': 33522,\n",
       " 'labrador,': 86288,\n",
       " 'stucky,': 34380,\n",
       " 'pelting': 101072,\n",
       " 'diminishing': 41912,\n",
       " 'cinematic': 33031,\n",
       " 'resonates': 67507,\n",
       " 'covets': 84477,\n",
       " 'friendly...': 29368,\n",
       " 'megatherium,[4]': 41780,\n",
       " 'chuckie.': 81881,\n",
       " 'cocked.': 13847,\n",
       " 'toscanini': 103124,\n",
       " 'machree.': 10847,\n",
       " 'spinning.': 54256,\n",
       " 'spinning,': 69980,\n",
       " 'practicing,': 39963,\n",
       " 'practicing.': 6467,\n",
       " \"no\\x97that's\": 43800,\n",
       " 'relaxation.': 12902,\n",
       " 'tech': 21177,\n",
       " 'fugitives': 27072,\n",
       " 'indignity,': 95301,\n",
       " 'affair.': 13476,\n",
       " 'affair,': 13722,\n",
       " 'dream....': 23995,\n",
       " 'saying': 1970,\n",
       " 'ponds.': 38598,\n",
       " 'dickey': 15215,\n",
       " 'tom...tom,': 57213,\n",
       " 'damaged.': 25234,\n",
       " 'martians.': 60596,\n",
       " 'damaged,': 77450,\n",
       " 'dicker': 30822,\n",
       " 'magico.': 68652,\n",
       " 'padded': 30121,\n",
       " 'mandy,': 95125,\n",
       " 'stay....': 84527,\n",
       " 'you...have': 20639,\n",
       " 'tempted': 9832,\n",
       " 'hounded': 57507,\n",
       " 'geek-boy.': 38415,\n",
       " 'bullshitted': 21441,\n",
       " 'clicked': 63270,\n",
       " 'think...did': 89032,\n",
       " 'blackface.': 15085,\n",
       " 'alignment...': 55436,\n",
       " 'religiosos': 74618,\n",
       " 'portchnik.': 63713,\n",
       " 'self-delusion.': 73195,\n",
       " 'other...': 4767,\n",
       " 'klaatu.': 58207,\n",
       " 'bodhi.': 95219,\n",
       " \"sayin'\": 11185,\n",
       " 'boeman': 38534,\n",
       " 'frankly.': 56389,\n",
       " 'frankly,': 3584,\n",
       " \"skipper's\": 60799,\n",
       " \"kid'\": 43737,\n",
       " 'plath': 64352,\n",
       " 'minimal.': 38421,\n",
       " 'iate': 86350,\n",
       " 'do...do': 79395,\n",
       " 'stay...i': 87383,\n",
       " 'altogether': 23022,\n",
       " 'massacring': 92529,\n",
       " 'migraines,': 104161,\n",
       " 'droning': 51396,\n",
       " 'jaguar': 64741,\n",
       " \"bankers'\": 10890,\n",
       " 'nicely': 20201,\n",
       " 'ajay.': 86664,\n",
       " 'patch': 14752,\n",
       " 'obscenely.': 46653,\n",
       " 'eighteen-minute': 47312,\n",
       " 'supermodel...': 45384,\n",
       " 'resigning.': 99237,\n",
       " 'circling': 10640,\n",
       " 'geronimo.': 59272,\n",
       " 'wh-wh-where': 93357,\n",
       " 'bushido,': 97010,\n",
       " 'tendon.': 25478,\n",
       " 'bet...': 4573,\n",
       " \"geronimo'\": 72839,\n",
       " 'thursday...': 45481,\n",
       " \"lappin'\": 65887,\n",
       " 'shoveling': 68793,\n",
       " 'pinot': 52531,\n",
       " 'main-frame': 31224,\n",
       " 'explain...': 4612,\n",
       " 'requisite.': 95885,\n",
       " 'etis': 72978,\n",
       " 'scouting,': 32140,\n",
       " \"barker's\": 63058,\n",
       " 'safely...': 55392,\n",
       " 'lots': 5003,\n",
       " 'irs': 36346,\n",
       " 'lodgings,': 58107,\n",
       " 'dissed': 27875,\n",
       " 'irk': 58104,\n",
       " 'ira': 96972,\n",
       " 'lota': 29115,\n",
       " 'with...': 5918,\n",
       " 'shithead,': 4190,\n",
       " 'wage': 82272,\n",
       " 'extend': 61556,\n",
       " 'nature': 5118,\n",
       " 'pino,': 75688,\n",
       " 'mm-hmm...': 97109,\n",
       " 'pino.': 75714,\n",
       " 'commies.': 107638,\n",
       " 'q47nr': 91936,\n",
       " 'ebonic-speaking': 15688,\n",
       " 'seriously.': 2828,\n",
       " 'extent': 6738,\n",
       " 'seriously,': 10101,\n",
       " 'casino...': 40147,\n",
       " \"heller's\": 53560,\n",
       " 'conan.': 101654,\n",
       " 'non-union': 75480,\n",
       " 'composite,': 90729,\n",
       " \"friends...we'll\": 69536,\n",
       " 'described.': 12680,\n",
       " 'scrape.': 107864,\n",
       " 'composite.': 13338,\n",
       " 'lookit': 27586,\n",
       " 'coulmier,': 50223,\n",
       " 'antiquities.': 35243,\n",
       " 'lot.': 6460,\n",
       " 'lookin': 39484,\n",
       " 'lot,': 5518,\n",
       " \"crazier'n\": 37447,\n",
       " 'raziel.': 109856,\n",
       " 'directly.': 23731,\n",
       " '...among': 48145,\n",
       " 'directly,': 50436,\n",
       " 'propellant.': 25456,\n",
       " 'libyan': 27520,\n",
       " 'twenty-three': 31160,\n",
       " 'chaser...': 60229,\n",
       " 'doubt,': 9933,\n",
       " 'doubt-': 62642,\n",
       " 'doubt.': 30675,\n",
       " 'gopher': 48246,\n",
       " \"bunty's...\": 11263,\n",
       " 'hooaaa': 92500,\n",
       " 'pericardial': 68272,\n",
       " 'humming': 10846,\n",
       " 'fra': 44454,\n",
       " 'underdone': 49169,\n",
       " 'pharmacological.': 86801,\n",
       " 'inta,': 78608,\n",
       " 'fro': 41090,\n",
       " 'laplante...': 83890,\n",
       " '.': 40,\n",
       " 'bothers': 4267,\n",
       " '...good-bye': 69633,\n",
       " 'apples,': 20968,\n",
       " 'much': 224,\n",
       " 'wiseguy': 28924,\n",
       " 'eclectic.': 64299,\n",
       " 'deliberately': 3626,\n",
       " \"i'm\\x97i'm\": 36630,\n",
       " 'ou...': 92335,\n",
       " 'karpoli': 69106,\n",
       " 'libya.': 95331,\n",
       " 'diets,': 67607,\n",
       " \"foreclosin'\": 48238,\n",
       " 'spit': 20076,\n",
       " 'precognition.': 94336,\n",
       " 'homies': 97613,\n",
       " 'almasy': 76885,\n",
       " 'spic': 27131,\n",
       " 'nettles.': 100080,\n",
       " 'doubts': 10187,\n",
       " \"hummin'\": 88437,\n",
       " \"pec's\": 78816,\n",
       " 'spin': 7821,\n",
       " 'keepers.': 50636,\n",
       " 'wildcat': 52796,\n",
       " \"kids've\": 94364,\n",
       " 'memory...and': 72761,\n",
       " 'slosh': 99679,\n",
       " 'grotty': 62013,\n",
       " 'wanted...': 17559,\n",
       " 'hudsucker,': 84853,\n",
       " \"boonies'll\": 48657,\n",
       " 'bother.': 5818,\n",
       " 'professionally': 36485,\n",
       " 'bother,': 17865,\n",
       " 'misconstrued': 99710,\n",
       " 'hullooo': 75235,\n",
       " 'chariot...': 30787,\n",
       " 'belittling': 64495,\n",
       " 'i-if-if': 82891,\n",
       " 'kumbaya': 83561,\n",
       " 'canoeing': 83604,\n",
       " 'spacers.': 17930,\n",
       " 'decade....': 104515,\n",
       " 'elctrolysis.': 34661,\n",
       " '...nobody': 70265,\n",
       " 'conditioned': 24528,\n",
       " 'orgasmic.': 79919,\n",
       " 'to-morrow': 96846,\n",
       " 'hyperdrive': 77518,\n",
       " 'conditioner': 171,\n",
       " 'plants.': 19447,\n",
       " 'desperately.': 92938,\n",
       " 'hone': 88074,\n",
       " 'memorial': 6383,\n",
       " \"excited'.\": 99717,\n",
       " 'himself...those': 81060,\n",
       " \"hubley's\": 8904,\n",
       " 'empathizing.': 45361,\n",
       " 'honk': 18783,\n",
       " 'mentor.': 36079,\n",
       " 'mentor,': 67323,\n",
       " 'penalty...': 100674,\n",
       " 'thing...all': 89072,\n",
       " 'provincials': 87976,\n",
       " 'dolphins...': 104698,\n",
       " 'cavanaugh': 79751,\n",
       " 'remans': 56148,\n",
       " \"jungle's\": 64792,\n",
       " 'hand-made.': 102256,\n",
       " 'chauvinistic': 97356,\n",
       " \"i-don't-know-what,\": 51401,\n",
       " 'frenchmen': 46941,\n",
       " 'assess,': 105938,\n",
       " 'torpedoes': 54660,\n",
       " 'spare...': 78218,\n",
       " 'beautify': 96913,\n",
       " \"i'd...\": 17854,\n",
       " 'reruns': 51171,\n",
       " 'edith...': 32471,\n",
       " 'collusion...': 101264,\n",
       " 'torpedoed': 55853,\n",
       " 'uno.': 72823,\n",
       " \"hon'\": 68402,\n",
       " 'reman.': 55968,\n",
       " 'furlough': 80817,\n",
       " 'provincial,': 71124,\n",
       " 'hon,': 38979,\n",
       " 'market-\\t\\t\\t\\t': 107099,\n",
       " 'hon.': 7801,\n",
       " 'spew.': 38525,\n",
       " 'woodworkers': 41870,\n",
       " 'academic': 4471,\n",
       " 'smells...': 103821,\n",
       " 'bop,': 107100,\n",
       " 'corporate': 5854,\n",
       " 'losers,': 12529,\n",
       " 'losers.': 29647,\n",
       " \"rogue's\": 106447,\n",
       " 'blond...gruner.': 35551,\n",
       " 'hushpuppies': 109551,\n",
       " 'venkman.': 29953,\n",
       " '...su-per': 110532,\n",
       " 'lasso': 59812,\n",
       " 'hah': 9207,\n",
       " 'hai': 42588,\n",
       " 'misdemeanor.': 70767,\n",
       " 'hal': 3300,\n",
       " '...ma': 4853,\n",
       " 'han': 35412,\n",
       " 'out-of-town': 38779,\n",
       " 'taxes.': 62745,\n",
       " 'taxes,': 21396,\n",
       " 'had': 51,\n",
       " 'advancement': 16178,\n",
       " 'haf': 67842,\n",
       " 'hag': 40759,\n",
       " 'hay': 7357,\n",
       " 'mcnamara': 61424,\n",
       " 'peculiarity.': 15636,\n",
       " 'jawed,': 75239,\n",
       " \"mozart's\": 10231,\n",
       " 'hap': 60156,\n",
       " 'har': 48331,\n",
       " 'has': 607,\n",
       " 'hat': 7252,\n",
       " '...my': 2361,\n",
       " 'haller.': 99106,\n",
       " 'municipal': 33659,\n",
       " 'fact...you': 74686,\n",
       " 'shuddup.': 44502,\n",
       " 'unequivocally': 56900,\n",
       " 'objective': 46622,\n",
       " 'taxation,': 67452,\n",
       " 'indicative': 32676,\n",
       " 'public\\x92': 105033,\n",
       " 'bello.': 57320,\n",
       " 'meatloaf,': 14106,\n",
       " 'lass.': 58164,\n",
       " 'sleuthing': 105685,\n",
       " 'dallying.': 95503,\n",
       " 'you\\x97made': 36619,\n",
       " 'ha,': 9565,\n",
       " 'ha-': 70249,\n",
       " 'ha.': 3093,\n",
       " 'veiwscreen,': 106906,\n",
       " '...hand': 39315,\n",
       " '...megan,': 80355,\n",
       " 'glazed': 95048,\n",
       " 'smiths.': 84143,\n",
       " 'but...then': 83016,\n",
       " 'fish....': 85731,\n",
       " 'sweet-n-low': 19083,\n",
       " 'plained': 86357,\n",
       " 'plan-b,': 69146,\n",
       " 'crowd': 3217,\n",
       " 'crowe': 9317,\n",
       " 'czech': 2225,\n",
       " 'breaktime.': 64371,\n",
       " 'crown': 14858,\n",
       " 'brigade.': 31731,\n",
       " 'snotbrains': 99088,\n",
       " \"yea...it's\": 108235,\n",
       " 'peterson.': 20770,\n",
       " 'peterson,': 68895,\n",
       " 'popular,': 52123,\n",
       " 'coddled': 101539,\n",
       " 'shoelaces.': 53970,\n",
       " \"are...i'm\": 86028,\n",
       " 'cahoon.': 92924,\n",
       " 'speeches,': 28652,\n",
       " 'perchance': 40829,\n",
       " 'bottom': 6441,\n",
       " 'lockdown': 31564,\n",
       " 'inhuman': 81485,\n",
       " \"cops'\": 107267,\n",
       " 'ribbon...': 108578,\n",
       " 'soon..': 91149,\n",
       " 'conditionals,': 105879,\n",
       " 'lawschool': 21413,\n",
       " 'lonely...': 56140,\n",
       " 'gunshots.': 66628,\n",
       " 'gunshots,': 98989,\n",
       " 'cokearama': 62019,\n",
       " 'crow.': 24330,\n",
       " 'curious....': 65635,\n",
       " 'culpa,': 47657,\n",
       " 'brigades': 33572,\n",
       " \"doesn't.\": 9582,\n",
       " 'starring': 10686,\n",
       " 'pilferage.': 93603,\n",
       " 'bamboo': 27553,\n",
       " 'ha\\x97': 49684,\n",
       " 'restlessness': 85494,\n",
       " 'benches': 40375,\n",
       " 'bud...': 61560,\n",
       " 'hisself.': 47742,\n",
       " 'hisself,': 107613,\n",
       " 'kilgore': 64855,\n",
       " 'socorro.': 56562,\n",
       " 'yelled,': 46682,\n",
       " 'university.': 23445,\n",
       " 'university,': 32432,\n",
       " 'leading.': 68871,\n",
       " 'marshall': 2395,\n",
       " 'honeymoon': 24379,\n",
       " \"stories...i've\": 104316,\n",
       " 'sassy.': 21457,\n",
       " 'marshals': 45913,\n",
       " 'could....': 85999,\n",
       " 'mangos': 64807,\n",
       " 'samoan...': 22579,\n",
       " 'shoots': 16647,\n",
       " 'perfume,': 27235,\n",
       " '...comes': 71434,\n",
       " 'perfume.': 17229,\n",
       " 'despised': 76608,\n",
       " 'fabric': 16798,\n",
       " 'brevoort': 50901,\n",
       " 'humble,': 62545,\n",
       " '1600,': 25964,\n",
       " 'raped': 5947,\n",
       " 'overnight.': 13556,\n",
       " 'grasping': 93201,\n",
       " 'despises': 21114,\n",
       " 'silvera.': 85760,\n",
       " 'rapes': 5892,\n",
       " 'plantoid': 54900,\n",
       " 'avedon,': 78020,\n",
       " 'shoot-': 71266,\n",
       " 'shoot,': 26228,\n",
       " 'catullus,': 30687,\n",
       " 'shoot.': 10840,\n",
       " 'marshal.': 54157,\n",
       " 'marshal,': 60517,\n",
       " 'distortions': 54436,\n",
       " 'entertained.': 36615,\n",
       " 'one-on-one.': 88772,\n",
       " 'sluggish...': 7511,\n",
       " 'grass...': 107441,\n",
       " 'incouraged': 97923,\n",
       " 'potomac...': 83120,\n",
       " 'tactics...': 60433,\n",
       " 'mcclusky': 92497,\n",
       " 'bridgework': 92866,\n",
       " \"guess...i'll\": 63241,\n",
       " 'incinerator': 21078,\n",
       " 'braces.': 21405,\n",
       " 'congratulations': 10134,\n",
       " \"shoal...'\": 74487,\n",
       " 'humbled': 27775,\n",
       " 'rape.': 5875,\n",
       " \"else's\": 4546,\n",
       " 'despise.': 68170,\n",
       " '50,000,': 45887,\n",
       " 'cooze,': 16808,\n",
       " 'worrying...': 20915,\n",
       " 'gregg.': 50617,\n",
       " \"else'd\": 47730,\n",
       " 'nicest': 19438,\n",
       " 'carey': 8247,\n",
       " 'piece-by-piece': 90172,\n",
       " \"physicist's\": 95984,\n",
       " 'alexandria,': 70040,\n",
       " 'alexandria.': 87893,\n",
       " 'passenger': 8134,\n",
       " 'dickering': 102839,\n",
       " 'explosion.': 9166,\n",
       " 'disgrace': 4007,\n",
       " 'and,\\twhat': 93229,\n",
       " 'yeah,,': 98992,\n",
       " \"strapped'\": 42689,\n",
       " 'forecourse.': 60722,\n",
       " 'decapitation': 60678,\n",
       " 'palm.': 14125,\n",
       " 'kane...': 69180,\n",
       " 'lore,': 53431,\n",
       " 'strapped,': 42623,\n",
       " 'lore.': 56027,\n",
       " 'slurring': 63190,\n",
       " 'biederman': 74986,\n",
       " 'contusions...': 51757,\n",
       " 'tampering.': 42343,\n",
       " 'badalato': 59943,\n",
       " 'crowns': 19742,\n",
       " 'stick-up-the-ass,': 44611,\n",
       " 'brightest,': 77490,\n",
       " 'pasadena': 68008,\n",
       " \"crush'em,\": 108821,\n",
       " 'but....but....': 83374,\n",
       " 'tape-recorded,': 106706,\n",
       " 'oyabun.': 17789,\n",
       " 'palms': 46980,\n",
       " 'leviathan,': 70688,\n",
       " 'leviathan.': 70543,\n",
       " 'uhm...': 40945,\n",
       " 'modern.': 32970,\n",
       " 'finish\\x97and': 37003,\n",
       " 'endangered.': 87194,\n",
       " 'smelling': 22805,\n",
       " 'beasley,': 97743,\n",
       " 'beasley.': 77926,\n",
       " \"feet'll\": 106229,\n",
       " 'explosions': 55668,\n",
       " 'loren': 57667,\n",
       " 'vulsellum': 21055,\n",
       " 'buuuuut.': 67001,\n",
       " 'seven-ten,': 100454,\n",
       " 'commended.': 99136,\n",
       " \"mike's\": 14247,\n",
       " \"driscoll's\": 102880,\n",
       " \"guy.'\": 28012,\n",
       " \"arcade's\": 91686,\n",
       " 'crown,': 97799,\n",
       " 'ofnatal': 110632,\n",
       " 'lyla.': 11502,\n",
       " \"coaches'\": 88291,\n",
       " 'rothstein.': 70274,\n",
       " 'childs': 16396,\n",
       " 'gloom.': 43268,\n",
       " 'chaim': 68034,\n",
       " 'chain': 5114,\n",
       " 'whoever': 1517,\n",
       " 'follow...': 48080,\n",
       " 'just.': 19982,\n",
       " 'like...ah...': 60579,\n",
       " '\\x97-i': 85543,\n",
       " 'embar-': 100979,\n",
       " 'melicertes.': 6425,\n",
       " 'putterer': 49797,\n",
       " 'chair': 11675,\n",
       " 'paycheck,': 88970,\n",
       " 'macht': 104344,\n",
       " 'ballet': 7436,\n",
       " 'amplification': 55035,\n",
       " 'acute,': 102987,\n",
       " 'curtis,': 28579,\n",
       " 'smashing,': 31800,\n",
       " 'freelance': 65210,\n",
       " 'smashing.': 67320,\n",
       " 'tomorrow...': 5009,\n",
       " 'balled': 31773,\n",
       " 'magazines.': 45456,\n",
       " 'psychoanalysts...': 85125,\n",
       " \"give'er\": 108830,\n",
       " \"reflexive'\": 110485,\n",
       " 'squad...': 102151,\n",
       " 'macho': 3852,\n",
       " 'oversight': 47488,\n",
       " \"eighty-two...who's\": 81095,\n",
       " 'tenacious': 43163,\n",
       " \"hearst's\": 50633,\n",
       " 'chai,': 72479,\n",
       " 'flatbed': 61577,\n",
       " 'jerk': 5497,\n",
       " 'self-destructed.': 63018,\n",
       " 'egypt,': 99020,\n",
       " 'egypt.': 36283,\n",
       " 'trieckonal': 50911,\n",
       " 'embark': 98738,\n",
       " 'gloomy': 26863,\n",
       " 'half-track,': 97688,\n",
       " 'head...ok': 89945,\n",
       " 'child,': 8848,\n",
       " 'child-': 103311,\n",
       " 'minute': 618,\n",
       " 'oath.': 30111,\n",
       " \"bag's\": 36930,\n",
       " 'good,': 444,\n",
       " '\\x97-\\t': 51858,\n",
       " 'apologize...': 6133,\n",
       " 'skewer': 40724,\n",
       " 'grapple,': 84607,\n",
       " 'agents...': 56866,\n",
       " 'nineties.': 13156,\n",
       " 'envy...': 63825,\n",
       " \"unsecured.'\": 49571,\n",
       " 'hindered': 83110,\n",
       " 'cockburn,': 104530,\n",
       " \"martha's\": 47302,\n",
       " 'h.q.,': 100927,\n",
       " 'heavyweight': 89668,\n",
       " 'chopping': 7362,\n",
       " 'climb.': 7002,\n",
       " 'accelerate...': 96311,\n",
       " 'climb,': 72313,\n",
       " '78s,': 29665,\n",
       " 'called...the': 103351,\n",
       " \"martha'd\": 98597,\n",
       " 'pools,': 101500,\n",
       " 'pools.': 48031,\n",
       " 'hudgeons,': 76587,\n",
       " 'celebrated': 9432,\n",
       " 'lynching': 92646,\n",
       " 'sultenfuss...': 92139,\n",
       " 'b-believe': 60111,\n",
       " 'bright...but': 90044,\n",
       " 'cabbage-eating': 67844,\n",
       " 'geography': 20959,\n",
       " 'crime...': 26645,\n",
       " 'jean-louis.': 71085,\n",
       " 'unintentionally': 74063,\n",
       " 'drafted': 6943,\n",
       " 'dealing...': 79799,\n",
       " 'degrees...': 25212,\n",
       " 'chere.': 42759,\n",
       " 'climbs': 33616,\n",
       " 'honour': 9657,\n",
       " 'plucking': 80737,\n",
       " 'risk...': 55358,\n",
       " 'thirties...': 67424,\n",
       " 'address': 2936,\n",
       " 'menjou': 102159,\n",
       " 'heap...': 78930,\n",
       " 'homes...': 74520,\n",
       " 'benson': 47073,\n",
       " 'celebrate.': 30916,\n",
       " 'celebrate,': 73633,\n",
       " 'manuscripts,': 37388,\n",
       " 'manuscripts.': 21669,\n",
       " 'reborn,': 40569,\n",
       " 'reborn.': 104395,\n",
       " 'nightstalker.': 6199,\n",
       " 'alfonse.': 21227,\n",
       " 'catch,': 86914,\n",
       " 'renata,': 59663,\n",
       " 'hotwiring': 51278,\n",
       " 'sprouted': 99634,\n",
       " 'ill-fated': 88170,\n",
       " 'degrees.': 12667,\n",
       " 'oooooo.': 14263,\n",
       " 'console,': 78515,\n",
       " 'console.': 59692,\n",
       " 'look...just': 107052,\n",
       " 'pencils.': 34362,\n",
       " 'pencils,': 95332,\n",
       " 'half-boy,': 75793,\n",
       " 'headed...': 87933,\n",
       " 'well...um...i': 101669,\n",
       " \"grandpa's\": 31506,\n",
       " 'slept.': 16389,\n",
       " 'annapuma.': 97342,\n",
       " 'chats,': 102866,\n",
       " 'toxoplasmosis': 106825,\n",
       " 'brando.': 51343,\n",
       " 'perished': 19689,\n",
       " 'favorable.': 108840,\n",
       " 'redman': 99396,\n",
       " \"guy'll\": 18013,\n",
       " 'risk.': 5847,\n",
       " 'opposed': 1073,\n",
       " ...}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dialogues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexed_dialogues=[]\n",
    "for line in cleaned_dialogues:\n",
    "    line_list=[]\n",
    "    for token in line:\n",
    "        try:\n",
    "            line_list.append(bag_of_words[token])\n",
    "        except:\n",
    "            line_list.append('0')\n",
    "    indexed_dialogues.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(indexed_dialogues[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues_dict={}\n",
    "line_number=0\n",
    "for line in indexed_dialogues:\n",
    "    dialogues_dict[line_number]=line\n",
    "    line_number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_keys=dialogues_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reply_keys=[]\n",
    "response_keys=[]\n",
    "for key in all_keys:\n",
    "    if(key%2==0):\n",
    "        reply_keys.append(key)\n",
    "    else:\n",
    "        response_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reply_index=0\n",
    "response_index=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogues_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lines=[]\n",
    "all_responses=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_lines=list(np.array(indexed_dialogues)[reply_keys])\n",
    "all_responses=list(np.array(indexed_dialogues)[response_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_length=0\n",
    "for line in all_lines:\n",
    "    total_length+=len(line)\n",
    "print total_length/len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_length=0\n",
    "for line in all_responses:\n",
    "    total_length+=len(line)\n",
    "print total_length/len(all_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152357"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_lines=[]\n",
    "sample_responses=[]\n",
    "for line in all_lines[20000:]:\n",
    "    sample_lines+=line\n",
    "    if(len(sample_lines)>=10000):\n",
    "        break\n",
    "for line in all_responses[20000:]:\n",
    "    sample_responses+=line\n",
    "    if(len(sample_responses)>=10000):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20003\n",
      "20007\n"
     ]
    }
   ],
   "source": [
    "print(len(sample_lines))\n",
    "print(len(sample_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  9990\n"
     ]
    }
   ],
   "source": [
    "seq_length = 10\n",
    "dataX = []\n",
    "dataY = []\n",
    "input_lines=[]\n",
    "output_line=[]\n",
    "for i in range(0, 10000-seq_length, 1):\n",
    "    input_line=sample_lines[i:i+seq_length]\n",
    "    output_line=sample_responses[i+seq_length]\n",
    "    dataX.append(input_line)\n",
    "    dataY.append(output_line)\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-3acb7b5b2712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gen_bag_of_words.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gen_index_to_words.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ISO-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_to_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('gen_bag_of_words.json', 'w', encoding = \"ISO-8859-1\") as fp:\n",
    "    json.dump(bag_of_words, fp)\n",
    "with open('gen_index_to_words.json', 'w',encoding = \"ISO-8859-1\") as fp:\n",
    "    json.dump(index_to_words, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(dataX, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (len(dataX), 10, 1))\n",
    "    # normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"Donna Files/text_generator_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 7.9181Epoch 00000: loss improved from inf to 7.91889, saving model to Donna Files/text_512_lstm_gen_weights-improvement-00-7.9189.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 7.9189    \n",
      "Epoch 2/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.7448Epoch 00001: loss improved from 7.91889 to 6.74486, saving model to Donna Files/text_512_lstm_gen_weights-improvement-01-6.7449.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.7449    \n",
      "Epoch 3/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.6251Epoch 00002: loss improved from 6.74486 to 6.62488, saving model to Donna Files/text_512_lstm_gen_weights-improvement-02-6.6249.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.6249    \n",
      "Epoch 4/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5773Epoch 00003: loss improved from 6.62488 to 6.57732, saving model to Donna Files/text_512_lstm_gen_weights-improvement-03-6.5773.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5773    \n",
      "Epoch 5/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5550Epoch 00004: loss improved from 6.57732 to 6.55501, saving model to Donna Files/text_512_lstm_gen_weights-improvement-04-6.5550.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5550    \n",
      "Epoch 6/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5443Epoch 00005: loss improved from 6.55501 to 6.54403, saving model to Donna Files/text_512_lstm_gen_weights-improvement-05-6.5440.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5440    \n",
      "Epoch 7/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5430Epoch 00006: loss improved from 6.54403 to 6.54282, saving model to Donna Files/text_512_lstm_gen_weights-improvement-06-6.5428.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5428    \n",
      "Epoch 8/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5410Epoch 00007: loss improved from 6.54282 to 6.54206, saving model to Donna Files/text_512_lstm_gen_weights-improvement-07-6.5421.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5421    \n",
      "Epoch 9/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5372Epoch 00008: loss improved from 6.54206 to 6.53689, saving model to Donna Files/text_512_lstm_gen_weights-improvement-08-6.5369.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5369    \n",
      "Epoch 10/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5339Epoch 00009: loss improved from 6.53689 to 6.53397, saving model to Donna Files/text_512_lstm_gen_weights-improvement-09-6.5340.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5340    \n",
      "Epoch 11/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5310Epoch 00010: loss improved from 6.53397 to 6.53160, saving model to Donna Files/text_512_lstm_gen_weights-improvement-10-6.5316.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5316    \n",
      "Epoch 12/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5320Epoch 00011: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5318    \n",
      "Epoch 13/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5280Epoch 00012: loss improved from 6.53160 to 6.52854, saving model to Donna Files/text_512_lstm_gen_weights-improvement-12-6.5285.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5285    \n",
      "Epoch 14/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5260Epoch 00013: loss improved from 6.52854 to 6.52548, saving model to Donna Files/text_512_lstm_gen_weights-improvement-13-6.5255.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5255    \n",
      "Epoch 15/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5251Epoch 00014: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5259    \n",
      "Epoch 16/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5231Epoch 00015: loss improved from 6.52548 to 6.52250, saving model to Donna Files/text_512_lstm_gen_weights-improvement-15-6.5225.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5225    \n",
      "Epoch 17/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5232Epoch 00016: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5226    \n",
      "Epoch 18/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5211Epoch 00017: loss improved from 6.52250 to 6.52036, saving model to Donna Files/text_512_lstm_gen_weights-improvement-17-6.5204.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5204    \n",
      "Epoch 19/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5202Epoch 00018: loss improved from 6.52036 to 6.52032, saving model to Donna Files/text_512_lstm_gen_weights-improvement-18-6.5203.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5203    \n",
      "Epoch 20/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5194Epoch 00019: loss improved from 6.52032 to 6.51877, saving model to Donna Files/text_512_lstm_gen_weights-improvement-19-6.5188.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5188    \n",
      "Epoch 21/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5204Epoch 00020: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5201    \n",
      "Epoch 22/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5184Epoch 00021: loss improved from 6.51877 to 6.51794, saving model to Donna Files/text_512_lstm_gen_weights-improvement-21-6.5179.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5179    \n",
      "Epoch 23/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5149Epoch 00022: loss improved from 6.51794 to 6.51473, saving model to Donna Files/text_512_lstm_gen_weights-improvement-22-6.5147.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5147    \n",
      "Epoch 24/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5151Epoch 00023: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5151    \n",
      "Epoch 25/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5144Epoch 00024: loss improved from 6.51473 to 6.51437, saving model to Donna Files/text_512_lstm_gen_weights-improvement-24-6.5144.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5144    \n",
      "Epoch 26/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5141Epoch 00025: loss improved from 6.51437 to 6.51373, saving model to Donna Files/text_512_lstm_gen_weights-improvement-25-6.5137.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5137    \n",
      "Epoch 27/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5116Epoch 00026: loss improved from 6.51373 to 6.51127, saving model to Donna Files/text_512_lstm_gen_weights-improvement-26-6.5113.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5113    \n",
      "Epoch 28/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5133Epoch 00027: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5139    \n",
      "Epoch 29/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5121Epoch 00028: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5124    \n",
      "Epoch 30/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5082Epoch 00029: loss improved from 6.51127 to 6.50851, saving model to Donna Files/text_512_lstm_gen_weights-improvement-29-6.5085.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.5085    \n",
      "Epoch 31/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5097Epoch 00030: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5095    \n",
      "Epoch 32/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5054Epoch 00031: loss improved from 6.50851 to 6.50599, saving model to Donna Files/text_512_lstm_gen_weights-improvement-31-6.5060.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.5060    \n",
      "Epoch 33/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5079Epoch 00032: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5087    \n",
      "Epoch 34/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5071Epoch 00033: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5072    \n",
      "Epoch 35/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5050Epoch 00034: loss improved from 6.50599 to 6.50507, saving model to Donna Files/text_512_lstm_gen_weights-improvement-34-6.5051.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.5051    \n",
      "Epoch 36/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5060Epoch 00035: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5055    \n",
      "Epoch 37/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5032Epoch 00036: loss improved from 6.50507 to 6.50319, saving model to Donna Files/text_512_lstm_gen_weights-improvement-36-6.5032.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.5032    \n",
      "Epoch 38/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5035Epoch 00037: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5033    \n",
      "Epoch 39/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5014Epoch 00038: loss improved from 6.50319 to 6.50183, saving model to Donna Files/text_512_lstm_gen_weights-improvement-38-6.5018.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.5018    \n",
      "Epoch 40/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5008Epoch 00039: loss improved from 6.50183 to 6.50112, saving model to Donna Files/text_512_lstm_gen_weights-improvement-39-6.5011.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.5011    \n",
      "Epoch 41/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5016Epoch 00040: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5017    \n",
      "Epoch 42/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.5014Epoch 00041: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.5020    \n",
      "Epoch 43/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4990Epoch 00042: loss improved from 6.50112 to 6.49879, saving model to Donna Files/text_512_lstm_gen_weights-improvement-42-6.4988.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4988    \n",
      "Epoch 44/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4982Epoch 00043: loss improved from 6.49879 to 6.49843, saving model to Donna Files/text_512_lstm_gen_weights-improvement-43-6.4984.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4984    \n",
      "Epoch 45/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4984Epoch 00044: loss improved from 6.49843 to 6.49782, saving model to Donna Files/text_512_lstm_gen_weights-improvement-44-6.4978.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4978    \n",
      "Epoch 46/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4977Epoch 00045: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 6.4979    \n",
      "Epoch 47/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4961Epoch 00046: loss improved from 6.49782 to 6.49550, saving model to Donna Files/text_512_lstm_gen_weights-improvement-46-6.4955.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4955    \n",
      "Epoch 48/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4904Epoch 00047: loss improved from 6.49550 to 6.49045, saving model to Donna Files/text_512_lstm_gen_weights-improvement-47-6.4904.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4904    \n",
      "Epoch 49/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4898Epoch 00048: loss improved from 6.49045 to 6.49023, saving model to Donna Files/text_512_lstm_gen_weights-improvement-48-6.4902.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4902    \n",
      "Epoch 50/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4865Epoch 00049: loss improved from 6.49023 to 6.48636, saving model to Donna Files/text_512_lstm_gen_weights-improvement-49-6.4864.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4864    \n",
      "Epoch 51/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4788Epoch 00050: loss improved from 6.48636 to 6.47903, saving model to Donna Files/text_512_lstm_gen_weights-improvement-50-6.4790.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4790    \n",
      "Epoch 52/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4695Epoch 00051: loss improved from 6.47903 to 6.47022, saving model to Donna Files/text_512_lstm_gen_weights-improvement-51-6.4702.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4702    \n",
      "Epoch 53/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4620Epoch 00052: loss improved from 6.47022 to 6.46253, saving model to Donna Files/text_512_lstm_gen_weights-improvement-52-6.4625.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4625    \n",
      "Epoch 54/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4484Epoch 00053: loss improved from 6.46253 to 6.44868, saving model to Donna Files/text_512_lstm_gen_weights-improvement-53-6.4487.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4487    \n",
      "Epoch 55/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4338Epoch 00054: loss improved from 6.44868 to 6.43337, saving model to Donna Files/text_512_lstm_gen_weights-improvement-54-6.4334.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4334    \n",
      "Epoch 56/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.4158Epoch 00055: loss improved from 6.43337 to 6.41587, saving model to Donna Files/text_512_lstm_gen_weights-improvement-55-6.4159.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.4159    \n",
      "Epoch 57/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.3932Epoch 00056: loss improved from 6.41587 to 6.39313, saving model to Donna Files/text_512_lstm_gen_weights-improvement-56-6.3931.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.3931    \n",
      "Epoch 58/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.3717Epoch 00057: loss improved from 6.39313 to 6.37128, saving model to Donna Files/text_512_lstm_gen_weights-improvement-57-6.3713.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.3713    \n",
      "Epoch 59/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.3386Epoch 00058: loss improved from 6.37128 to 6.33903, saving model to Donna Files/text_512_lstm_gen_weights-improvement-58-6.3390.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.3390    \n",
      "Epoch 60/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.3024Epoch 00059: loss improved from 6.33903 to 6.30246, saving model to Donna Files/text_512_lstm_gen_weights-improvement-59-6.3025.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.3025    \n",
      "Epoch 61/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.2654Epoch 00060: loss improved from 6.30246 to 6.26554, saving model to Donna Files/text_512_lstm_gen_weights-improvement-60-6.2655.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.2655    \n",
      "Epoch 62/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.2245Epoch 00061: loss improved from 6.26554 to 6.22466, saving model to Donna Files/text_512_lstm_gen_weights-improvement-61-6.2247.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.2247    \n",
      "Epoch 63/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.1927Epoch 00062: loss improved from 6.22466 to 6.19281, saving model to Donna Files/text_512_lstm_gen_weights-improvement-62-6.1928.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.1928    \n",
      "Epoch 64/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.1526Epoch 00063: loss improved from 6.19281 to 6.15265, saving model to Donna Files/text_512_lstm_gen_weights-improvement-63-6.1526.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.1526    \n",
      "Epoch 65/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.1142Epoch 00064: loss improved from 6.15265 to 6.11388, saving model to Donna Files/text_512_lstm_gen_weights-improvement-64-6.1139.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.1139    \n",
      "Epoch 66/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.0733Epoch 00065: loss improved from 6.11388 to 6.07358, saving model to Donna Files/text_512_lstm_gen_weights-improvement-65-6.0736.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 6.0736    \n",
      "Epoch 67/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 6.0225Epoch 00066: loss improved from 6.07358 to 6.02333, saving model to Donna Files/text_512_lstm_gen_weights-improvement-66-6.0233.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 6.0233    \n",
      "Epoch 68/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.9833Epoch 00067: loss improved from 6.02333 to 5.98352, saving model to Donna Files/text_512_lstm_gen_weights-improvement-67-5.9835.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.9835    \n",
      "Epoch 69/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.9375Epoch 00068: loss improved from 5.98352 to 5.93782, saving model to Donna Files/text_512_lstm_gen_weights-improvement-68-5.9378.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.9378    \n",
      "Epoch 70/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.8922Epoch 00069: loss improved from 5.93782 to 5.89201, saving model to Donna Files/text_512_lstm_gen_weights-improvement-69-5.8920.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.8920    \n",
      "Epoch 71/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.8344Epoch 00070: loss improved from 5.89201 to 5.83484, saving model to Donna Files/text_512_lstm_gen_weights-improvement-70-5.8348.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.8348    \n",
      "Epoch 72/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.7868Epoch 00071: loss improved from 5.83484 to 5.78727, saving model to Donna Files/text_512_lstm_gen_weights-improvement-71-5.7873.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 5.7873    \n",
      "Epoch 73/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.7263Epoch 00072: loss improved from 5.78727 to 5.72653, saving model to Donna Files/text_512_lstm_gen_weights-improvement-72-5.7265.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 5.7265    \n",
      "Epoch 74/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.6714Epoch 00073: loss improved from 5.72653 to 5.67114, saving model to Donna Files/text_512_lstm_gen_weights-improvement-73-5.6711.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.6711    \n",
      "Epoch 75/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.6080Epoch 00074: loss improved from 5.67114 to 5.60874, saving model to Donna Files/text_512_lstm_gen_weights-improvement-74-5.6087.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.6087    \n",
      "Epoch 76/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.5420Epoch 00075: loss improved from 5.60874 to 5.54129, saving model to Donna Files/text_512_lstm_gen_weights-improvement-75-5.5413.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.5413    \n",
      "Epoch 77/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.4809Epoch 00076: loss improved from 5.54129 to 5.48088, saving model to Donna Files/text_512_lstm_gen_weights-improvement-76-5.4809.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.4809    \n",
      "Epoch 78/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.4117Epoch 00077: loss improved from 5.48088 to 5.41210, saving model to Donna Files/text_512_lstm_gen_weights-improvement-77-5.4121.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.4121    \n",
      "Epoch 79/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.3446Epoch 00078: loss improved from 5.41210 to 5.34473, saving model to Donna Files/text_512_lstm_gen_weights-improvement-78-5.3447.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.3447    \n",
      "Epoch 80/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.2710Epoch 00079: loss improved from 5.34473 to 5.27132, saving model to Donna Files/text_512_lstm_gen_weights-improvement-79-5.2713.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.2713    \n",
      "Epoch 81/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.2128Epoch 00080: loss improved from 5.27132 to 5.21324, saving model to Donna Files/text_512_lstm_gen_weights-improvement-80-5.2132.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.2132    \n",
      "Epoch 82/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.1540Epoch 00081: loss improved from 5.21324 to 5.15409, saving model to Donna Files/text_512_lstm_gen_weights-improvement-81-5.1541.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.1541    \n",
      "Epoch 83/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.0809Epoch 00082: loss improved from 5.15409 to 5.08133, saving model to Donna Files/text_512_lstm_gen_weights-improvement-82-5.0813.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.0813    \n",
      "Epoch 84/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 5.0192Epoch 00083: loss improved from 5.08133 to 5.01880, saving model to Donna Files/text_512_lstm_gen_weights-improvement-83-5.0188.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 5.0188    \n",
      "Epoch 85/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.9516Epoch 00084: loss improved from 5.01880 to 4.95166, saving model to Donna Files/text_512_lstm_gen_weights-improvement-84-4.9517.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.9517    \n",
      "Epoch 86/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.8842Epoch 00085: loss improved from 4.95166 to 4.88403, saving model to Donna Files/text_512_lstm_gen_weights-improvement-85-4.8840.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.8840    \n",
      "Epoch 87/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.8409Epoch 00086: loss improved from 4.88403 to 4.84150, saving model to Donna Files/text_512_lstm_gen_weights-improvement-86-4.8415.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.8415    \n",
      "Epoch 88/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.7851Epoch 00087: loss improved from 4.84150 to 4.78550, saving model to Donna Files/text_512_lstm_gen_weights-improvement-87-4.7855.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.7855    \n",
      "Epoch 89/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.7128Epoch 00088: loss improved from 4.78550 to 4.71278, saving model to Donna Files/text_512_lstm_gen_weights-improvement-88-4.7128.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.7128    \n",
      "Epoch 90/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.6527Epoch 00089: loss improved from 4.71278 to 4.65266, saving model to Donna Files/text_512_lstm_gen_weights-improvement-89-4.6527.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.6527    \n",
      "Epoch 91/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.5941Epoch 00090: loss improved from 4.65266 to 4.59419, saving model to Donna Files/text_512_lstm_gen_weights-improvement-90-4.5942.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.5942    \n",
      "Epoch 92/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.5435Epoch 00091: loss improved from 4.59419 to 4.54379, saving model to Donna Files/text_512_lstm_gen_weights-improvement-91-4.5438.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.5438    \n",
      "Epoch 93/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.4815Epoch 00092: loss improved from 4.54379 to 4.48172, saving model to Donna Files/text_512_lstm_gen_weights-improvement-92-4.4817.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.4817    \n",
      "Epoch 94/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.4219Epoch 00093: loss improved from 4.48172 to 4.42202, saving model to Donna Files/text_512_lstm_gen_weights-improvement-93-4.4220.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.4220    \n",
      "Epoch 95/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.3518Epoch 00094: loss improved from 4.42202 to 4.35216, saving model to Donna Files/text_512_lstm_gen_weights-improvement-94-4.3522.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.3522    \n",
      "Epoch 96/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.2964Epoch 00095: loss improved from 4.35216 to 4.29639, saving model to Donna Files/text_512_lstm_gen_weights-improvement-95-4.2964.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.2964    \n",
      "Epoch 97/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.2438Epoch 00096: loss improved from 4.29639 to 4.24359, saving model to Donna Files/text_512_lstm_gen_weights-improvement-96-4.2436.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.2436    \n",
      "Epoch 98/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.1915Epoch 00097: loss improved from 4.24359 to 4.19147, saving model to Donna Files/text_512_lstm_gen_weights-improvement-97-4.1915.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.1915    \n",
      "Epoch 99/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.1370Epoch 00098: loss improved from 4.19147 to 4.13656, saving model to Donna Files/text_512_lstm_gen_weights-improvement-98-4.1366.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.1366    \n",
      "Epoch 100/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.0761Epoch 00099: loss improved from 4.13656 to 4.07579, saving model to Donna Files/text_512_lstm_gen_weights-improvement-99-4.0758.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.0758    \n",
      "Epoch 101/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 4.0204Epoch 00100: loss improved from 4.07579 to 4.02042, saving model to Donna Files/text_512_lstm_gen_weights-improvement-100-4.0204.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 4.0204    \n",
      "Epoch 102/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.9704Epoch 00101: loss improved from 4.02042 to 3.97057, saving model to Donna Files/text_512_lstm_gen_weights-improvement-101-3.9706.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.9706    \n",
      "Epoch 103/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.9350Epoch 00102: loss improved from 3.97057 to 3.93523, saving model to Donna Files/text_512_lstm_gen_weights-improvement-102-3.9352.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.9352    \n",
      "Epoch 104/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.8946Epoch 00103: loss improved from 3.93523 to 3.89432, saving model to Donna Files/text_512_lstm_gen_weights-improvement-103-3.8943.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.8943    \n",
      "Epoch 105/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.8207Epoch 00104: loss improved from 3.89432 to 3.82106, saving model to Donna Files/text_512_lstm_gen_weights-improvement-104-3.8211.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.8211    \n",
      "Epoch 106/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.7803Epoch 00105: loss improved from 3.82106 to 3.78007, saving model to Donna Files/text_512_lstm_gen_weights-improvement-105-3.7801.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.7801    \n",
      "Epoch 107/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.7182Epoch 00106: loss improved from 3.78007 to 3.71850, saving model to Donna Files/text_512_lstm_gen_weights-improvement-106-3.7185.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.7185    \n",
      "Epoch 108/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.6691Epoch 00107: loss improved from 3.71850 to 3.66879, saving model to Donna Files/text_512_lstm_gen_weights-improvement-107-3.6688.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.6688    \n",
      "Epoch 109/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.6250Epoch 00108: loss improved from 3.66879 to 3.62530, saving model to Donna Files/text_512_lstm_gen_weights-improvement-108-3.6253.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.6253    \n",
      "Epoch 110/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.5951Epoch 00109: loss improved from 3.62530 to 3.59519, saving model to Donna Files/text_512_lstm_gen_weights-improvement-109-3.5952.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.5952    \n",
      "Epoch 111/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.5522Epoch 00110: loss improved from 3.59519 to 3.55245, saving model to Donna Files/text_512_lstm_gen_weights-improvement-110-3.5525.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.5525    \n",
      "Epoch 112/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.5023Epoch 00111: loss improved from 3.55245 to 3.50239, saving model to Donna Files/text_512_lstm_gen_weights-improvement-111-3.5024.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.5024    \n",
      "Epoch 113/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.4538Epoch 00112: loss improved from 3.50239 to 3.45486, saving model to Donna Files/text_512_lstm_gen_weights-improvement-112-3.4549.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.4549    \n",
      "Epoch 114/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.4343Epoch 00113: loss improved from 3.45486 to 3.43411, saving model to Donna Files/text_512_lstm_gen_weights-improvement-113-3.4341.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.4341    \n",
      "Epoch 115/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.3946Epoch 00114: loss improved from 3.43411 to 3.39470, saving model to Donna Files/text_512_lstm_gen_weights-improvement-114-3.3947.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.3947    \n",
      "Epoch 116/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.3401Epoch 00115: loss improved from 3.39470 to 3.33963, saving model to Donna Files/text_512_lstm_gen_weights-improvement-115-3.3396.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.3396    \n",
      "Epoch 117/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.3004Epoch 00116: loss improved from 3.33963 to 3.30042, saving model to Donna Files/text_512_lstm_gen_weights-improvement-116-3.3004.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.3004    \n",
      "Epoch 118/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.2593Epoch 00117: loss improved from 3.30042 to 3.25861, saving model to Donna Files/text_512_lstm_gen_weights-improvement-117-3.2586.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.2586    \n",
      "Epoch 119/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.2187Epoch 00118: loss improved from 3.25861 to 3.21787, saving model to Donna Files/text_512_lstm_gen_weights-improvement-118-3.2179.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.2179    \n",
      "Epoch 120/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.1915Epoch 00119: loss improved from 3.21787 to 3.19147, saving model to Donna Files/text_512_lstm_gen_weights-improvement-119-3.1915.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.1915    \n",
      "Epoch 121/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.1694Epoch 00120: loss improved from 3.19147 to 3.17028, saving model to Donna Files/text_512_lstm_gen_weights-improvement-120-3.1703.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 3.1703    \n",
      "Epoch 122/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.1578Epoch 00121: loss improved from 3.17028 to 3.15721, saving model to Donna Files/text_512_lstm_gen_weights-improvement-121-3.1572.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.1572    \n",
      "Epoch 123/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.0755Epoch 00122: loss improved from 3.15721 to 3.07592, saving model to Donna Files/text_512_lstm_gen_weights-improvement-122-3.0759.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.0759    \n",
      "Epoch 124/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.0724Epoch 00123: loss improved from 3.07592 to 3.07269, saving model to Donna Files/text_512_lstm_gen_weights-improvement-123-3.0727.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.0727    \n",
      "Epoch 125/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 3.0053Epoch 00124: loss improved from 3.07269 to 3.00566, saving model to Donna Files/text_512_lstm_gen_weights-improvement-124-3.0057.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 3.0057    \n",
      "Epoch 126/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.9893Epoch 00125: loss improved from 3.00566 to 2.98941, saving model to Donna Files/text_512_lstm_gen_weights-improvement-125-2.9894.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.9894    \n",
      "Epoch 127/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.9467Epoch 00126: loss improved from 2.98941 to 2.94557, saving model to Donna Files/text_512_lstm_gen_weights-improvement-126-2.9456.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.9456    \n",
      "Epoch 128/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.9145Epoch 00127: loss improved from 2.94557 to 2.91430, saving model to Donna Files/text_512_lstm_gen_weights-improvement-127-2.9143.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.9143    \n",
      "Epoch 129/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.8991Epoch 00128: loss improved from 2.91430 to 2.89943, saving model to Donna Files/text_512_lstm_gen_weights-improvement-128-2.8994.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.8994    \n",
      "Epoch 130/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.8633Epoch 00129: loss improved from 2.89943 to 2.86278, saving model to Donna Files/text_512_lstm_gen_weights-improvement-129-2.8628.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.8628    \n",
      "Epoch 131/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.8440Epoch 00130: loss improved from 2.86278 to 2.84337, saving model to Donna Files/text_512_lstm_gen_weights-improvement-130-2.8434.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.8434    \n",
      "Epoch 132/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.7791Epoch 00131: loss improved from 2.84337 to 2.77981, saving model to Donna Files/text_512_lstm_gen_weights-improvement-131-2.7798.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.7798    \n",
      "Epoch 133/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.7880Epoch 00132: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 2.7882    \n",
      "Epoch 134/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.7601Epoch 00133: loss improved from 2.77981 to 2.76058, saving model to Donna Files/text_512_lstm_gen_weights-improvement-133-2.7606.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.7606    \n",
      "Epoch 135/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.7142Epoch 00134: loss improved from 2.76058 to 2.71424, saving model to Donna Files/text_512_lstm_gen_weights-improvement-134-2.7142.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.7142    \n",
      "Epoch 136/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.6923Epoch 00135: loss improved from 2.71424 to 2.69261, saving model to Donna Files/text_512_lstm_gen_weights-improvement-135-2.6926.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.6926    \n",
      "Epoch 137/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.6559Epoch 00136: loss improved from 2.69261 to 2.65619, saving model to Donna Files/text_512_lstm_gen_weights-improvement-136-2.6562.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.6562    \n",
      "Epoch 138/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.6442Epoch 00137: loss improved from 2.65619 to 2.64473, saving model to Donna Files/text_512_lstm_gen_weights-improvement-137-2.6447.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.6447    \n",
      "Epoch 139/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.6018Epoch 00138: loss improved from 2.64473 to 2.60125, saving model to Donna Files/text_512_lstm_gen_weights-improvement-138-2.6012.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.6012    \n",
      "Epoch 140/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.5748Epoch 00139: loss improved from 2.60125 to 2.57434, saving model to Donna Files/text_512_lstm_gen_weights-improvement-139-2.5743.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.5743    \n",
      "Epoch 141/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.5547Epoch 00140: loss improved from 2.57434 to 2.55480, saving model to Donna Files/text_512_lstm_gen_weights-improvement-140-2.5548.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.5548    \n",
      "Epoch 142/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.5225Epoch 00141: loss improved from 2.55480 to 2.52244, saving model to Donna Files/text_512_lstm_gen_weights-improvement-141-2.5224.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.5224    \n",
      "Epoch 143/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.5006Epoch 00142: loss improved from 2.52244 to 2.50034, saving model to Donna Files/text_512_lstm_gen_weights-improvement-142-2.5003.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.5003    \n",
      "Epoch 144/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.4766Epoch 00143: loss improved from 2.50034 to 2.47693, saving model to Donna Files/text_512_lstm_gen_weights-improvement-143-2.4769.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.4769    \n",
      "Epoch 145/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.4376Epoch 00144: loss improved from 2.47693 to 2.43792, saving model to Donna Files/text_512_lstm_gen_weights-improvement-144-2.4379.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.4379    \n",
      "Epoch 146/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.4365Epoch 00145: loss improved from 2.43792 to 2.43652, saving model to Donna Files/text_512_lstm_gen_weights-improvement-145-2.4365.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.4365    \n",
      "Epoch 147/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.3936Epoch 00146: loss improved from 2.43652 to 2.39375, saving model to Donna Files/text_512_lstm_gen_weights-improvement-146-2.3937.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.3937    \n",
      "Epoch 148/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.3832Epoch 00147: loss improved from 2.39375 to 2.38351, saving model to Donna Files/text_512_lstm_gen_weights-improvement-147-2.3835.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.3835    \n",
      "Epoch 149/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.3518Epoch 00148: loss improved from 2.38351 to 2.35199, saving model to Donna Files/text_512_lstm_gen_weights-improvement-148-2.3520.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.3520    \n",
      "Epoch 150/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.3367Epoch 00149: loss improved from 2.35199 to 2.33679, saving model to Donna Files/text_512_lstm_gen_weights-improvement-149-2.3368.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.3368    \n",
      "Epoch 151/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.3176Epoch 00150: loss improved from 2.33679 to 2.31764, saving model to Donna Files/text_512_lstm_gen_weights-improvement-150-2.3176.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.3176    \n",
      "Epoch 152/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.2882Epoch 00151: loss improved from 2.31764 to 2.28833, saving model to Donna Files/text_512_lstm_gen_weights-improvement-151-2.2883.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.2883    \n",
      "Epoch 153/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.2534Epoch 00152: loss improved from 2.28833 to 2.25345, saving model to Donna Files/text_512_lstm_gen_weights-improvement-152-2.2534.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.2534    \n",
      "Epoch 154/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.2290Epoch 00153: loss improved from 2.25345 to 2.22877, saving model to Donna Files/text_512_lstm_gen_weights-improvement-153-2.2288.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.2288    \n",
      "Epoch 155/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.2239Epoch 00154: loss improved from 2.22877 to 2.22400, saving model to Donna Files/text_512_lstm_gen_weights-improvement-154-2.2240.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.2240    \n",
      "Epoch 156/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1889Epoch 00155: loss improved from 2.22400 to 2.18850, saving model to Donna Files/text_512_lstm_gen_weights-improvement-155-2.1885.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.1885    \n",
      "Epoch 157/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1726Epoch 00156: loss improved from 2.18850 to 2.17296, saving model to Donna Files/text_512_lstm_gen_weights-improvement-156-2.1730.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.1730    \n",
      "Epoch 158/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1582Epoch 00157: loss improved from 2.17296 to 2.15805, saving model to Donna Files/text_512_lstm_gen_weights-improvement-157-2.1580.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.1580    \n",
      "Epoch 159/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1493Epoch 00158: loss improved from 2.15805 to 2.14985, saving model to Donna Files/text_512_lstm_gen_weights-improvement-158-2.1498.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.1498    \n",
      "Epoch 160/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1073Epoch 00159: loss improved from 2.14985 to 2.10800, saving model to Donna Files/text_512_lstm_gen_weights-improvement-159-2.1080.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.1080    \n",
      "Epoch 161/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.1318Epoch 00160: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 2.1311    \n",
      "Epoch 162/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.0773Epoch 00161: loss improved from 2.10800 to 2.07742, saving model to Donna Files/text_512_lstm_gen_weights-improvement-161-2.0774.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.0774    \n",
      "Epoch 163/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.0751Epoch 00162: loss improved from 2.07742 to 2.07492, saving model to Donna Files/text_512_lstm_gen_weights-improvement-162-2.0749.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.0749    \n",
      "Epoch 164/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.0086Epoch 00163: loss improved from 2.07492 to 2.00823, saving model to Donna Files/text_512_lstm_gen_weights-improvement-163-2.0082.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 2.0082    \n",
      "Epoch 165/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.0255Epoch 00164: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 2.0260    \n",
      "Epoch 166/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 2.0025Epoch 00165: loss improved from 2.00823 to 2.00184, saving model to Donna Files/text_512_lstm_gen_weights-improvement-165-2.0018.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 2.0018    \n",
      "Epoch 167/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9686Epoch 00166: loss improved from 2.00184 to 1.96882, saving model to Donna Files/text_512_lstm_gen_weights-improvement-166-1.9688.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.9688    \n",
      "Epoch 168/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9633Epoch 00167: loss improved from 1.96882 to 1.96357, saving model to Donna Files/text_512_lstm_gen_weights-improvement-167-1.9636.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.9636    \n",
      "Epoch 169/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9713Epoch 00168: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.9719    \n",
      "Epoch 170/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9424Epoch 00169: loss improved from 1.96357 to 1.94234, saving model to Donna Files/text_512_lstm_gen_weights-improvement-169-1.9423.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.9423    \n",
      "Epoch 171/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9013Epoch 00170: loss improved from 1.94234 to 1.90104, saving model to Donna Files/text_512_lstm_gen_weights-improvement-170-1.9010.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.9010    \n",
      "Epoch 172/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.9198Epoch 00171: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.9199    \n",
      "Epoch 173/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8650Epoch 00172: loss improved from 1.90104 to 1.86446, saving model to Donna Files/text_512_lstm_gen_weights-improvement-172-1.8645.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.8645    \n",
      "Epoch 174/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8484Epoch 00173: loss improved from 1.86446 to 1.84847, saving model to Donna Files/text_512_lstm_gen_weights-improvement-173-1.8485.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.8485    \n",
      "Epoch 175/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8403Epoch 00174: loss improved from 1.84847 to 1.84008, saving model to Donna Files/text_512_lstm_gen_weights-improvement-174-1.8401.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.8401    \n",
      "Epoch 176/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8168Epoch 00175: loss improved from 1.84008 to 1.81698, saving model to Donna Files/text_512_lstm_gen_weights-improvement-175-1.8170.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 1.8170    \n",
      "Epoch 177/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8056Epoch 00176: loss improved from 1.81698 to 1.80592, saving model to Donna Files/text_512_lstm_gen_weights-improvement-176-1.8059.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.8059    \n",
      "Epoch 178/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.8000Epoch 00177: loss improved from 1.80592 to 1.79973, saving model to Donna Files/text_512_lstm_gen_weights-improvement-177-1.7997.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.7997    \n",
      "Epoch 179/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.7687Epoch 00178: loss improved from 1.79973 to 1.76829, saving model to Donna Files/text_512_lstm_gen_weights-improvement-178-1.7683.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.7683    \n",
      "Epoch 180/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.7566Epoch 00179: loss improved from 1.76829 to 1.75592, saving model to Donna Files/text_512_lstm_gen_weights-improvement-179-1.7559.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.7559    \n",
      "Epoch 181/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.7312Epoch 00180: loss improved from 1.75592 to 1.73160, saving model to Donna Files/text_512_lstm_gen_weights-improvement-180-1.7316.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.7316    \n",
      "Epoch 182/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.7598Epoch 00181: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.7599    \n",
      "Epoch 183/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.7272Epoch 00182: loss improved from 1.73160 to 1.72704, saving model to Donna Files/text_512_lstm_gen_weights-improvement-182-1.7270.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 1.7270    \n",
      "Epoch 184/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6775Epoch 00183: loss improved from 1.72704 to 1.67760, saving model to Donna Files/text_512_lstm_gen_weights-improvement-183-1.6776.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.6776    \n",
      "Epoch 185/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6920Epoch 00184: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.6924    \n",
      "Epoch 186/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6755Epoch 00185: loss improved from 1.67760 to 1.67519, saving model to Donna Files/text_512_lstm_gen_weights-improvement-185-1.6752.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 1.6752    \n",
      "Epoch 187/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6672Epoch 00186: loss improved from 1.67519 to 1.66747, saving model to Donna Files/text_512_lstm_gen_weights-improvement-186-1.6675.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.6675    \n",
      "Epoch 188/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6546Epoch 00187: loss improved from 1.66747 to 1.65440, saving model to Donna Files/text_512_lstm_gen_weights-improvement-187-1.6544.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.6544    \n",
      "Epoch 189/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6109Epoch 00188: loss improved from 1.65440 to 1.61034, saving model to Donna Files/text_512_lstm_gen_weights-improvement-188-1.6103.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.6103    \n",
      "Epoch 190/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5904Epoch 00189: loss improved from 1.61034 to 1.59019, saving model to Donna Files/text_512_lstm_gen_weights-improvement-189-1.5902.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5902    \n",
      "Epoch 191/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.6109Epoch 00190: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.6106    \n",
      "Epoch 192/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5820Epoch 00191: loss improved from 1.59019 to 1.58193, saving model to Donna Files/text_512_lstm_gen_weights-improvement-191-1.5819.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5819    \n",
      "Epoch 193/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5686Epoch 00192: loss improved from 1.58193 to 1.56836, saving model to Donna Files/text_512_lstm_gen_weights-improvement-192-1.5684.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5684    \n",
      "Epoch 194/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5391Epoch 00193: loss improved from 1.56836 to 1.53960, saving model to Donna Files/text_512_lstm_gen_weights-improvement-193-1.5396.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5396    \n",
      "Epoch 195/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5337Epoch 00194: loss improved from 1.53960 to 1.53324, saving model to Donna Files/text_512_lstm_gen_weights-improvement-194-1.5332.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5332    \n",
      "Epoch 196/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5125Epoch 00195: loss improved from 1.53324 to 1.51266, saving model to Donna Files/text_512_lstm_gen_weights-improvement-195-1.5127.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.5127    \n",
      "Epoch 197/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4984Epoch 00196: loss improved from 1.51266 to 1.49829, saving model to Donna Files/text_512_lstm_gen_weights-improvement-196-1.4983.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.4983    \n",
      "Epoch 198/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5037Epoch 00197: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.5034    \n",
      "Epoch 199/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4688Epoch 00198: loss improved from 1.49829 to 1.46921, saving model to Donna Files/text_512_lstm_gen_weights-improvement-198-1.4692.hdf5\n",
      "9990/9990 [==============================] - 30s - loss: 1.4692    \n",
      "Epoch 200/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4728Epoch 00199: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.4732    \n",
      "Epoch 201/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.5001Epoch 00200: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.5004    \n",
      "Epoch 202/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4614Epoch 00201: loss improved from 1.46921 to 1.46146, saving model to Donna Files/text_512_lstm_gen_weights-improvement-201-1.4615.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 1.4615    \n",
      "Epoch 203/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4347Epoch 00202: loss improved from 1.46146 to 1.43451, saving model to Donna Files/text_512_lstm_gen_weights-improvement-202-1.4345.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 1.4345    \n",
      "Epoch 204/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4364Epoch 00203: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.4368    \n",
      "Epoch 205/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4054Epoch 00204: loss improved from 1.43451 to 1.40504, saving model to Donna Files/text_512_lstm_gen_weights-improvement-204-1.4050.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.4050    \n",
      "Epoch 206/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.4105Epoch 00205: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.4101    \n",
      "Epoch 207/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3683Epoch 00206: loss improved from 1.40504 to 1.36812, saving model to Donna Files/text_512_lstm_gen_weights-improvement-206-1.3681.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.3681    \n",
      "Epoch 208/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3821Epoch 00207: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3818    \n",
      "Epoch 209/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3668Epoch 00208: loss improved from 1.36812 to 1.36702, saving model to Donna Files/text_512_lstm_gen_weights-improvement-208-1.3670.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.3670    \n",
      "Epoch 210/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3732Epoch 00209: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3733    \n",
      "Epoch 211/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3694Epoch 00210: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3696    \n",
      "Epoch 212/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3568Epoch 00211: loss improved from 1.36702 to 1.35669, saving model to Donna Files/text_512_lstm_gen_weights-improvement-211-1.3567.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.3567    \n",
      "Epoch 213/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3035Epoch 00212: loss improved from 1.35669 to 1.30338, saving model to Donna Files/text_512_lstm_gen_weights-improvement-212-1.3034.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.3034    \n",
      "Epoch 214/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2954Epoch 00213: loss improved from 1.30338 to 1.29530, saving model to Donna Files/text_512_lstm_gen_weights-improvement-213-1.2953.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.2953    \n",
      "Epoch 215/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3090Epoch 00214: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3086    \n",
      "Epoch 216/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2529Epoch 00215: loss improved from 1.29530 to 1.25301, saving model to Donna Files/text_512_lstm_gen_weights-improvement-215-1.2530.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.2530    \n",
      "Epoch 217/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3184Epoch 00216: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3193    \n",
      "Epoch 218/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2564Epoch 00217: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.2564    \n",
      "Epoch 219/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2611Epoch 00218: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.2609    \n",
      "Epoch 220/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2364Epoch 00219: loss improved from 1.25301 to 1.23616, saving model to Donna Files/text_512_lstm_gen_weights-improvement-219-1.2362.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.2362    \n",
      "Epoch 221/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2125Epoch 00220: loss improved from 1.23616 to 1.21271, saving model to Donna Files/text_512_lstm_gen_weights-improvement-220-1.2127.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.2127    \n",
      "Epoch 222/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2387Epoch 00221: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.2391    \n",
      "Epoch 223/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2483Epoch 00222: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.2484    \n",
      "Epoch 224/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.3136Epoch 00223: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.3131    \n",
      "Epoch 225/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1854Epoch 00224: loss improved from 1.21271 to 1.18506, saving model to Donna Files/text_512_lstm_gen_weights-improvement-224-1.1851.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1851    \n",
      "Epoch 226/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.2231Epoch 00225: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.2232    \n",
      "Epoch 227/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1672Epoch 00226: loss improved from 1.18506 to 1.16825, saving model to Donna Files/text_512_lstm_gen_weights-improvement-226-1.1682.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1682    \n",
      "Epoch 228/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1895Epoch 00227: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.1897    \n",
      "Epoch 229/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1584Epoch 00228: loss improved from 1.16825 to 1.15867, saving model to Donna Files/text_512_lstm_gen_weights-improvement-228-1.1587.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1587    \n",
      "Epoch 230/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1412Epoch 00229: loss improved from 1.15867 to 1.14119, saving model to Donna Files/text_512_lstm_gen_weights-improvement-229-1.1412.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1412    \n",
      "Epoch 231/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1421Epoch 00230: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.1420    \n",
      "Epoch 232/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1194Epoch 00231: loss improved from 1.14119 to 1.11991, saving model to Donna Files/text_512_lstm_gen_weights-improvement-231-1.1199.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1199    \n",
      "Epoch 233/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1412Epoch 00232: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.1417    \n",
      "Epoch 234/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.1063Epoch 00233: loss improved from 1.11991 to 1.10625, saving model to Donna Files/text_512_lstm_gen_weights-improvement-233-1.1063.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.1063    \n",
      "Epoch 235/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0993Epoch 00234: loss improved from 1.10625 to 1.09955, saving model to Donna Files/text_512_lstm_gen_weights-improvement-234-1.0996.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0996    \n",
      "Epoch 236/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0899Epoch 00235: loss improved from 1.09955 to 1.09043, saving model to Donna Files/text_512_lstm_gen_weights-improvement-235-1.0904.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0904    \n",
      "Epoch 237/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0905Epoch 00236: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0910    \n",
      "Epoch 238/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0678Epoch 00237: loss improved from 1.09043 to 1.06801, saving model to Donna Files/text_512_lstm_gen_weights-improvement-237-1.0680.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0680    \n",
      "Epoch 239/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0773Epoch 00238: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0772    \n",
      "Epoch 240/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0473Epoch 00239: loss improved from 1.06801 to 1.04696, saving model to Donna Files/text_512_lstm_gen_weights-improvement-239-1.0470.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0470    \n",
      "Epoch 241/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0354Epoch 00240: loss improved from 1.04696 to 1.03584, saving model to Donna Files/text_512_lstm_gen_weights-improvement-240-1.0358.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0358    \n",
      "Epoch 242/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0470Epoch 00241: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0465    \n",
      "Epoch 243/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0133Epoch 00242: loss improved from 1.03584 to 1.01358, saving model to Donna Files/text_512_lstm_gen_weights-improvement-242-1.0136.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0136    \n",
      "Epoch 244/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0933Epoch 00243: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0935    \n",
      "Epoch 245/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0404Epoch 00244: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0403    \n",
      "Epoch 246/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0183Epoch 00245: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 1.0185    \n",
      "Epoch 247/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 1.0106Epoch 00246: loss improved from 1.01358 to 1.01055, saving model to Donna Files/text_512_lstm_gen_weights-improvement-246-1.0105.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 1.0105    \n",
      "Epoch 248/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9944Epoch 00247: loss improved from 1.01055 to 0.99435, saving model to Donna Files/text_512_lstm_gen_weights-improvement-247-0.9943.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.9943    \n",
      "Epoch 249/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9779Epoch 00248: loss improved from 0.99435 to 0.97798, saving model to Donna Files/text_512_lstm_gen_weights-improvement-248-0.9780.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9780    \n",
      "Epoch 250/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9721Epoch 00249: loss improved from 0.97798 to 0.97240, saving model to Donna Files/text_512_lstm_gen_weights-improvement-249-0.9724.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9724    \n",
      "Epoch 251/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9619Epoch 00250: loss improved from 0.97240 to 0.96216, saving model to Donna Files/text_512_lstm_gen_weights-improvement-250-0.9622.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9622    \n",
      "Epoch 252/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9804Epoch 00251: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9802    \n",
      "Epoch 253/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9442Epoch 00252: loss improved from 0.96216 to 0.94423, saving model to Donna Files/text_512_lstm_gen_weights-improvement-252-0.9442.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9442    \n",
      "Epoch 254/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9579Epoch 00253: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9585    \n",
      "Epoch 255/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9865Epoch 00254: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9865    \n",
      "Epoch 256/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9391Epoch 00255: loss improved from 0.94423 to 0.94020, saving model to Donna Files/text_512_lstm_gen_weights-improvement-255-0.9402.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9402    \n",
      "Epoch 257/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9302Epoch 00256: loss improved from 0.94020 to 0.93054, saving model to Donna Files/text_512_lstm_gen_weights-improvement-256-0.9305.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9305    \n",
      "Epoch 258/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9344Epoch 00257: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9351    \n",
      "Epoch 259/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9240Epoch 00258: loss improved from 0.93054 to 0.92365, saving model to Donna Files/text_512_lstm_gen_weights-improvement-258-0.9237.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9237    \n",
      "Epoch 260/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9009Epoch 00259: loss improved from 0.92365 to 0.90102, saving model to Donna Files/text_512_lstm_gen_weights-improvement-259-0.9010.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.9010    \n",
      "Epoch 261/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8962Epoch 00260: loss improved from 0.90102 to 0.89593, saving model to Donna Files/text_512_lstm_gen_weights-improvement-260-0.8959.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8959    \n",
      "Epoch 262/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8896Epoch 00261: loss improved from 0.89593 to 0.88961, saving model to Donna Files/text_512_lstm_gen_weights-improvement-261-0.8896.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8896    \n",
      "Epoch 263/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8898Epoch 00262: loss improved from 0.88961 to 0.88936, saving model to Donna Files/text_512_lstm_gen_weights-improvement-262-0.8894.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8894    \n",
      "Epoch 264/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8613Epoch 00263: loss improved from 0.88936 to 0.86136, saving model to Donna Files/text_512_lstm_gen_weights-improvement-263-0.8614.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8614    \n",
      "Epoch 265/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8976Epoch 00264: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8981    \n",
      "Epoch 266/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8525Epoch 00265: loss improved from 0.86136 to 0.85227, saving model to Donna Files/text_512_lstm_gen_weights-improvement-265-0.8523.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8523    \n",
      "Epoch 267/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8563Epoch 00266: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8563    \n",
      "Epoch 268/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9277Epoch 00267: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9276    \n",
      "Epoch 269/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8920Epoch 00268: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8918    \n",
      "Epoch 270/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8235Epoch 00269: loss improved from 0.85227 to 0.82333, saving model to Donna Files/text_512_lstm_gen_weights-improvement-269-0.8233.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8233    \n",
      "Epoch 271/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8147Epoch 00270: loss improved from 0.82333 to 0.81476, saving model to Donna Files/text_512_lstm_gen_weights-improvement-270-0.8148.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8148    \n",
      "Epoch 272/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8151Epoch 00271: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8153    \n",
      "Epoch 273/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8536Epoch 00272: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8534    \n",
      "Epoch 274/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8107Epoch 00273: loss improved from 0.81476 to 0.81053, saving model to Donna Files/text_512_lstm_gen_weights-improvement-273-0.8105.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8105    \n",
      "Epoch 275/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8061Epoch 00274: loss improved from 0.81053 to 0.80597, saving model to Donna Files/text_512_lstm_gen_weights-improvement-274-0.8060.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.8060    \n",
      "Epoch 276/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7872Epoch 00275: loss improved from 0.80597 to 0.78731, saving model to Donna Files/text_512_lstm_gen_weights-improvement-275-0.7873.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7873    \n",
      "Epoch 277/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8015Epoch 00276: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8021    \n",
      "Epoch 278/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.9054Epoch 00277: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.9052    \n",
      "Epoch 279/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7647Epoch 00278: loss improved from 0.78731 to 0.76474, saving model to Donna Files/text_512_lstm_gen_weights-improvement-278-0.7647.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7647    \n",
      "Epoch 280/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8107Epoch 00279: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8113    \n",
      "Epoch 281/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8279Epoch 00280: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8276    \n",
      "Epoch 282/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7509Epoch 00281: loss improved from 0.76474 to 0.75090, saving model to Donna Files/text_512_lstm_gen_weights-improvement-281-0.7509.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.7509    \n",
      "Epoch 283/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7546Epoch 00282: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7545    \n",
      "Epoch 284/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7826Epoch 00283: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7826    \n",
      "Epoch 285/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7422Epoch 00284: loss improved from 0.75090 to 0.74234, saving model to Donna Files/text_512_lstm_gen_weights-improvement-284-0.7423.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7423    \n",
      "Epoch 286/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7378Epoch 00285: loss improved from 0.74234 to 0.73745, saving model to Donna Files/text_512_lstm_gen_weights-improvement-285-0.7375.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7375    \n",
      "Epoch 287/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7151Epoch 00286: loss improved from 0.73745 to 0.71535, saving model to Donna Files/text_512_lstm_gen_weights-improvement-286-0.7153.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7153    \n",
      "Epoch 288/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7324Epoch 00287: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7321    \n",
      "Epoch 289/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7080Epoch 00288: loss improved from 0.71535 to 0.70809, saving model to Donna Files/text_512_lstm_gen_weights-improvement-288-0.7081.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.7081    \n",
      "Epoch 290/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7367Epoch 00289: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7365    \n",
      "Epoch 291/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8339Epoch 00290: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8344    \n",
      "Epoch 292/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7878Epoch 00291: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7882    \n",
      "Epoch 293/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7322Epoch 00292: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7325    \n",
      "Epoch 294/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7320Epoch 00293: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7318    \n",
      "Epoch 295/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6883Epoch 00294: loss improved from 0.70809 to 0.68833, saving model to Donna Files/text_512_lstm_gen_weights-improvement-294-0.6883.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6883    \n",
      "Epoch 296/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6780Epoch 00295: loss improved from 0.68833 to 0.67790, saving model to Donna Files/text_512_lstm_gen_weights-improvement-295-0.6779.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6779    \n",
      "Epoch 297/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6624Epoch 00296: loss improved from 0.67790 to 0.66211, saving model to Donna Files/text_512_lstm_gen_weights-improvement-296-0.6621.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6621    \n",
      "Epoch 298/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6626Epoch 00297: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6623    \n",
      "Epoch 299/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6364Epoch 00298: loss improved from 0.66211 to 0.63659, saving model to Donna Files/text_512_lstm_gen_weights-improvement-298-0.6366.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6366    \n",
      "Epoch 300/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6378Epoch 00299: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6381    \n",
      "Epoch 301/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6612Epoch 00300: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6618    \n",
      "Epoch 302/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7297Epoch 00301: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7299    \n",
      "Epoch 303/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7434Epoch 00302: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7432    \n",
      "Epoch 304/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6664Epoch 00303: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6661    \n",
      "Epoch 305/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6228Epoch 00304: loss improved from 0.63659 to 0.62276, saving model to Donna Files/text_512_lstm_gen_weights-improvement-304-0.6228.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6228    \n",
      "Epoch 306/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6302Epoch 00305: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.6304    \n",
      "Epoch 307/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.7920Epoch 00306: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.7924    \n",
      "Epoch 308/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6481Epoch 00307: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6477    \n",
      "Epoch 309/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6230Epoch 00308: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6228    \n",
      "Epoch 310/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6118Epoch 00309: loss improved from 0.62276 to 0.61175, saving model to Donna Files/text_512_lstm_gen_weights-improvement-309-0.6117.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.6117    \n",
      "Epoch 311/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5978Epoch 00310: loss improved from 0.61175 to 0.59747, saving model to Donna Files/text_512_lstm_gen_weights-improvement-310-0.5975.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5975    \n",
      "Epoch 312/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5848Epoch 00311: loss improved from 0.59747 to 0.58482, saving model to Donna Files/text_512_lstm_gen_weights-improvement-311-0.5848.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5848    \n",
      "Epoch 313/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6300Epoch 00312: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6298    \n",
      "Epoch 314/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5964Epoch 00313: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5963    \n",
      "Epoch 315/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6049Epoch 00314: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6048    \n",
      "Epoch 316/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6294Epoch 00315: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6297    \n",
      "Epoch 317/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6668Epoch 00316: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6665    \n",
      "Epoch 318/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5864Epoch 00317: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5865    \n",
      "Epoch 319/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6102Epoch 00318: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6104    \n",
      "Epoch 320/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6196Epoch 00319: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6197    \n",
      "Epoch 321/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5734Epoch 00320: loss improved from 0.58482 to 0.57361, saving model to Donna Files/text_512_lstm_gen_weights-improvement-320-0.5736.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5736    \n",
      "Epoch 322/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5837Epoch 00321: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5844    \n",
      "Epoch 323/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5793Epoch 00322: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5794    \n",
      "Epoch 324/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5756Epoch 00323: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5759    \n",
      "Epoch 325/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5997Epoch 00324: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5994    \n",
      "Epoch 326/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5553Epoch 00325: loss improved from 0.57361 to 0.55507, saving model to Donna Files/text_512_lstm_gen_weights-improvement-325-0.5551.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5551    \n",
      "Epoch 327/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5180Epoch 00326: loss improved from 0.55507 to 0.51783, saving model to Donna Files/text_512_lstm_gen_weights-improvement-326-0.5178.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5178    \n",
      "Epoch 328/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5353Epoch 00327: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5351    \n",
      "Epoch 329/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5438Epoch 00328: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5436    \n",
      "Epoch 330/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5248Epoch 00329: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5248    \n",
      "Epoch 331/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5711Epoch 00330: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5713    \n",
      "Epoch 332/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5522Epoch 00331: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5530    \n",
      "Epoch 333/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6418Epoch 00332: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6415    \n",
      "Epoch 334/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5105Epoch 00333: loss improved from 0.51783 to 0.51046, saving model to Donna Files/text_512_lstm_gen_weights-improvement-333-0.5105.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5105    \n",
      "Epoch 335/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5080Epoch 00334: loss improved from 0.51046 to 0.50774, saving model to Donna Files/text_512_lstm_gen_weights-improvement-334-0.5077.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.5077    \n",
      "Epoch 336/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5173Epoch 00335: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5171    \n",
      "Epoch 337/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5121Epoch 00336: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5120    \n",
      "Epoch 338/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5083Epoch 00337: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5080    \n",
      "Epoch 339/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4889Epoch 00338: loss improved from 0.50774 to 0.48870, saving model to Donna Files/text_512_lstm_gen_weights-improvement-338-0.4887.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4887    \n",
      "Epoch 340/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4897Epoch 00339: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4895    \n",
      "Epoch 341/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4975Epoch 00340: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4976    \n",
      "Epoch 342/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5794Epoch 00341: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5792    \n",
      "Epoch 343/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5053Epoch 00342: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5056    \n",
      "Epoch 344/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5210Epoch 00343: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5208    \n",
      "Epoch 345/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4760Epoch 00344: loss improved from 0.48870 to 0.47640, saving model to Donna Files/text_512_lstm_gen_weights-improvement-344-0.4764.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4764    \n",
      "Epoch 346/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5236Epoch 00345: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5241    \n",
      "Epoch 347/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5025Epoch 00346: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5023    \n",
      "Epoch 348/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4858Epoch 00347: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4861    \n",
      "Epoch 349/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.8857Epoch 00348: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.8854    \n",
      "Epoch 350/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4805Epoch 00349: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4808    \n",
      "Epoch 351/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5403Epoch 00350: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5401    \n",
      "Epoch 352/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4515Epoch 00351: loss improved from 0.47640 to 0.45174, saving model to Donna Files/text_512_lstm_gen_weights-improvement-351-0.4517.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4517    \n",
      "Epoch 353/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4762Epoch 00352: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4761    \n",
      "Epoch 354/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5075Epoch 00353: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5078    \n",
      "Epoch 355/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4488Epoch 00354: loss improved from 0.45174 to 0.44895, saving model to Donna Files/text_512_lstm_gen_weights-improvement-354-0.4489.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4489    \n",
      "Epoch 356/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4805Epoch 00355: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4803    \n",
      "Epoch 357/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4271Epoch 00356: loss improved from 0.44895 to 0.42730, saving model to Donna Files/text_512_lstm_gen_weights-improvement-356-0.4273.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4273    \n",
      "Epoch 358/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4297Epoch 00357: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4294    \n",
      "Epoch 359/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4182Epoch 00358: loss improved from 0.42730 to 0.41817, saving model to Donna Files/text_512_lstm_gen_weights-improvement-358-0.4182.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.4182    \n",
      "Epoch 360/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4240Epoch 00359: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4239    \n",
      "Epoch 361/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4507Epoch 00360: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4505    \n",
      "Epoch 362/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4173Epoch 00361: loss improved from 0.41817 to 0.41723, saving model to Donna Files/text_512_lstm_gen_weights-improvement-361-0.4172.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4172    \n",
      "Epoch 363/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4102Epoch 00362: loss improved from 0.41723 to 0.41043, saving model to Donna Files/text_512_lstm_gen_weights-improvement-362-0.4104.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4104    \n",
      "Epoch 364/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4505Epoch 00363: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4504    \n",
      "Epoch 365/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4139Epoch 00364: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4137    \n",
      "Epoch 366/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4060Epoch 00365: loss improved from 0.41043 to 0.40603, saving model to Donna Files/text_512_lstm_gen_weights-improvement-365-0.4060.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.4060    \n",
      "Epoch 367/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3968Epoch 00366: loss improved from 0.40603 to 0.39664, saving model to Donna Files/text_512_lstm_gen_weights-improvement-366-0.3966.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3966    \n",
      "Epoch 368/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4149Epoch 00367: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4150    \n",
      "Epoch 369/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.5276Epoch 00368: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.5274    \n",
      "Epoch 370/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4142Epoch 00369: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4140    \n",
      "Epoch 371/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4165Epoch 00370: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4168    \n",
      "Epoch 372/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4472Epoch 00371: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4473    \n",
      "Epoch 373/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4243Epoch 00372: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4242    \n",
      "Epoch 374/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4138Epoch 00373: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4136    \n",
      "Epoch 375/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3905Epoch 00374: loss improved from 0.39664 to 0.39030, saving model to Donna Files/text_512_lstm_gen_weights-improvement-374-0.3903.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3903    \n",
      "Epoch 376/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3754Epoch 00375: loss improved from 0.39030 to 0.37524, saving model to Donna Files/text_512_lstm_gen_weights-improvement-375-0.3752.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3752    \n",
      "Epoch 377/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3810Epoch 00376: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3809    \n",
      "Epoch 378/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3961Epoch 00377: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3959    \n",
      "Epoch 379/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3840Epoch 00378: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3841    \n",
      "Epoch 380/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4074Epoch 00379: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4075    \n",
      "Epoch 381/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3686Epoch 00380: loss improved from 0.37524 to 0.36841, saving model to Donna Files/text_512_lstm_gen_weights-improvement-380-0.3684.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3684    \n",
      "Epoch 382/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3685Epoch 00381: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3686    \n",
      "Epoch 383/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3660Epoch 00382: loss improved from 0.36841 to 0.36594, saving model to Donna Files/text_512_lstm_gen_weights-improvement-382-0.3659.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3659    \n",
      "Epoch 384/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3756Epoch 00383: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3763    \n",
      "Epoch 385/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4133Epoch 00384: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4131    \n",
      "Epoch 386/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3855Epoch 00385: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3854    \n",
      "Epoch 387/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3860Epoch 00386: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3858    \n",
      "Epoch 388/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3733Epoch 00387: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3732    \n",
      "Epoch 389/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3666Epoch 00388: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3671    \n",
      "Epoch 390/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4035Epoch 00389: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4034    \n",
      "Epoch 391/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3801Epoch 00390: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3801    \n",
      "Epoch 392/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3791Epoch 00391: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3798    \n",
      "Epoch 393/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4613Epoch 00392: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4612    \n",
      "Epoch 394/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3569Epoch 00393: loss improved from 0.36594 to 0.35689, saving model to Donna Files/text_512_lstm_gen_weights-improvement-393-0.3569.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3569    \n",
      "Epoch 395/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3938Epoch 00394: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3936    \n",
      "Epoch 396/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3504Epoch 00395: loss improved from 0.35689 to 0.35054, saving model to Donna Files/text_512_lstm_gen_weights-improvement-395-0.3505.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3505    \n",
      "Epoch 397/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3618Epoch 00396: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3618    \n",
      "Epoch 398/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3481Epoch 00397: loss improved from 0.35054 to 0.34809, saving model to Donna Files/text_512_lstm_gen_weights-improvement-397-0.3481.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.3481    \n",
      "Epoch 399/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3471Epoch 00398: loss improved from 0.34809 to 0.34711, saving model to Donna Files/text_512_lstm_gen_weights-improvement-398-0.3471.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3471    \n",
      "Epoch 400/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3567Epoch 00399: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3567    \n",
      "Epoch 401/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3388Epoch 00400: loss improved from 0.34711 to 0.33875, saving model to Donna Files/text_512_lstm_gen_weights-improvement-400-0.3387.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3387    \n",
      "Epoch 402/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4036Epoch 00401: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4038    \n",
      "Epoch 403/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3848Epoch 00402: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3846    \n",
      "Epoch 404/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3331Epoch 00403: loss improved from 0.33875 to 0.33290, saving model to Donna Files/text_512_lstm_gen_weights-improvement-403-0.3329.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3329    \n",
      "Epoch 405/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3190Epoch 00404: loss improved from 0.33290 to 0.31897, saving model to Donna Files/text_512_lstm_gen_weights-improvement-404-0.3190.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3190    \n",
      "Epoch 406/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3340Epoch 00405: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3339    \n",
      "Epoch 407/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3157Epoch 00406: loss improved from 0.31897 to 0.31552, saving model to Donna Files/text_512_lstm_gen_weights-improvement-406-0.3155.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3155    \n",
      "Epoch 408/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3178Epoch 00407: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3181    \n",
      "Epoch 409/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4021Epoch 00408: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4020    \n",
      "Epoch 410/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3261Epoch 00409: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3263    \n",
      "Epoch 411/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3404Epoch 00410: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3409    \n",
      "Epoch 412/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3897Epoch 00411: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3895    \n",
      "Epoch 413/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3276Epoch 00412: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3275    \n",
      "Epoch 414/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3245Epoch 00413: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3245    \n",
      "Epoch 415/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3349Epoch 00414: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3349    \n",
      "Epoch 416/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3043Epoch 00415: loss improved from 0.31552 to 0.30434, saving model to Donna Files/text_512_lstm_gen_weights-improvement-415-0.3043.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.3043    \n",
      "Epoch 417/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3238Epoch 00416: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3237    \n",
      "Epoch 418/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3119Epoch 00417: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3121    \n",
      "Epoch 419/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3168Epoch 00418: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3167    \n",
      "Epoch 420/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2789Epoch 00419: loss improved from 0.30434 to 0.27891, saving model to Donna Files/text_512_lstm_gen_weights-improvement-419-0.2789.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2789    \n",
      "Epoch 421/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2999Epoch 00420: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2998    \n",
      "Epoch 422/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2627Epoch 00421: loss improved from 0.27891 to 0.26257, saving model to Donna Files/text_512_lstm_gen_weights-improvement-421-0.2626.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2626    \n",
      "Epoch 423/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2859Epoch 00422: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2863    \n",
      "Epoch 424/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3192Epoch 00423: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3191    \n",
      "Epoch 425/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3123Epoch 00424: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3122    \n",
      "Epoch 426/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2801Epoch 00425: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2801    \n",
      "Epoch 427/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2766Epoch 00426: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2767    \n",
      "Epoch 428/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2713Epoch 00427: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2719    \n",
      "Epoch 429/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3706Epoch 00428: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3704    \n",
      "Epoch 430/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2701Epoch 00429: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2702    \n",
      "Epoch 431/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2788Epoch 00430: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2788    \n",
      "Epoch 432/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2639Epoch 00431: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2642    \n",
      "Epoch 433/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3411Epoch 00432: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3412    \n",
      "Epoch 434/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3265Epoch 00433: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3266    \n",
      "Epoch 435/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2695Epoch 00434: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2697    \n",
      "Epoch 436/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2892Epoch 00435: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2891    \n",
      "Epoch 437/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2757Epoch 00436: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2758    \n",
      "Epoch 438/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2921Epoch 00437: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2919    \n",
      "Epoch 439/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2507Epoch 00438: loss improved from 0.26257 to 0.25078, saving model to Donna Files/text_512_lstm_gen_weights-improvement-438-0.2508.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2508    \n",
      "Epoch 440/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2680Epoch 00439: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2678    \n",
      "Epoch 441/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2587Epoch 00440: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2588    \n",
      "Epoch 442/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2767Epoch 00441: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2766    \n",
      "Epoch 443/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3022Epoch 00442: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3021    \n",
      "Epoch 444/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2593Epoch 00443: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2593    \n",
      "Epoch 445/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2517Epoch 00444: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2516    \n",
      "Epoch 446/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2499Epoch 00445: loss improved from 0.25078 to 0.24985, saving model to Donna Files/text_512_lstm_gen_weights-improvement-445-0.2499.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2499    \n",
      "Epoch 447/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2442Epoch 00446: loss improved from 0.24985 to 0.24420, saving model to Donna Files/text_512_lstm_gen_weights-improvement-446-0.2442.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2442    \n",
      "Epoch 448/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3134Epoch 00447: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3137    \n",
      "Epoch 449/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3587Epoch 00448: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3586    \n",
      "Epoch 450/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2638Epoch 00449: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2641    \n",
      "Epoch 451/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3127Epoch 00450: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3126    \n",
      "Epoch 452/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2549Epoch 00451: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2552    \n",
      "Epoch 453/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3581Epoch 00452: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3580    \n",
      "Epoch 454/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2490Epoch 00453: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2491    \n",
      "Epoch 455/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2743Epoch 00454: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2741    \n",
      "Epoch 456/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2399Epoch 00455: loss improved from 0.24420 to 0.23977, saving model to Donna Files/text_512_lstm_gen_weights-improvement-455-0.2398.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2398    \n",
      "Epoch 457/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2201Epoch 00456: loss improved from 0.23977 to 0.21997, saving model to Donna Files/text_512_lstm_gen_weights-improvement-456-0.2200.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2200    \n",
      "Epoch 458/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2133Epoch 00457: loss improved from 0.21997 to 0.21352, saving model to Donna Files/text_512_lstm_gen_weights-improvement-457-0.2135.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2135    \n",
      "Epoch 459/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2484Epoch 00458: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2482    \n",
      "Epoch 460/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2123Epoch 00459: loss improved from 0.21352 to 0.21217, saving model to Donna Files/text_512_lstm_gen_weights-improvement-459-0.2122.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2122    \n",
      "Epoch 461/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2151Epoch 00460: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2150    \n",
      "Epoch 462/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2229Epoch 00461: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2229    \n",
      "Epoch 463/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2086Epoch 00462: loss improved from 0.21217 to 0.20900, saving model to Donna Files/text_512_lstm_gen_weights-improvement-462-0.2090.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.2090    \n",
      "Epoch 464/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2940Epoch 00463: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2940    \n",
      "Epoch 465/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2443Epoch 00464: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2445    \n",
      "Epoch 466/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2837Epoch 00465: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2838    \n",
      "Epoch 467/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3952Epoch 00466: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3956    \n",
      "Epoch 468/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3058Epoch 00467: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3058    \n",
      "Epoch 469/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2275Epoch 00468: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2275    \n",
      "Epoch 470/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2271Epoch 00469: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2269    \n",
      "Epoch 471/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2532Epoch 00470: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2532    \n",
      "Epoch 472/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2169Epoch 00471: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2168    \n",
      "Epoch 473/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2135Epoch 00472: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2134    \n",
      "Epoch 474/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1984Epoch 00473: loss improved from 0.20900 to 0.19833, saving model to Donna Files/text_512_lstm_gen_weights-improvement-473-0.1983.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1983    \n",
      "Epoch 475/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2113Epoch 00474: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2112    \n",
      "Epoch 476/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2041Epoch 00475: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2041    \n",
      "Epoch 477/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2171Epoch 00476: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2170    \n",
      "Epoch 478/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2348Epoch 00477: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2348    \n",
      "Epoch 479/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1952Epoch 00478: loss improved from 0.19833 to 0.19517, saving model to Donna Files/text_512_lstm_gen_weights-improvement-478-0.1952.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1952    \n",
      "Epoch 480/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2112Epoch 00479: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2111    \n",
      "Epoch 481/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2325Epoch 00480: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2323    \n",
      "Epoch 482/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1922Epoch 00481: loss improved from 0.19517 to 0.19214, saving model to Donna Files/text_512_lstm_gen_weights-improvement-481-0.1921.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1921    \n",
      "Epoch 483/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2045Epoch 00482: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2044    \n",
      "Epoch 484/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2073Epoch 00483: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2072    \n",
      "Epoch 485/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2002Epoch 00484: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2004    \n",
      "Epoch 486/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2773Epoch 00485: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2773    \n",
      "Epoch 487/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2234Epoch 00486: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2236    \n",
      "Epoch 488/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2270Epoch 00487: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2269    \n",
      "Epoch 489/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1903Epoch 00488: loss improved from 0.19214 to 0.19071, saving model to Donna Files/text_512_lstm_gen_weights-improvement-488-0.1907.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1907    \n",
      "Epoch 490/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.6521Epoch 00489: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.6520    \n",
      "Epoch 491/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3167Epoch 00490: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3167    \n",
      "Epoch 492/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2430Epoch 00491: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2428    \n",
      "Epoch 493/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1807Epoch 00492: loss improved from 0.19071 to 0.18067, saving model to Donna Files/text_512_lstm_gen_weights-improvement-492-0.1807.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1807    \n",
      "Epoch 494/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1790Epoch 00493: loss improved from 0.18067 to 0.17902, saving model to Donna Files/text_512_lstm_gen_weights-improvement-493-0.1790.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1790    \n",
      "Epoch 495/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1908Epoch 00494: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1907    \n",
      "Epoch 496/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1858Epoch 00495: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1859    \n",
      "Epoch 497/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1869Epoch 00496: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1871    \n",
      "Epoch 498/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2051Epoch 00497: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2050    \n",
      "Epoch 499/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1943Epoch 00498: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1944    \n",
      "Epoch 500/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2667Epoch 00499: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2666    \n",
      "Epoch 501/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1913Epoch 00500: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1912    \n",
      "Epoch 502/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1705Epoch 00501: loss improved from 0.17902 to 0.17045, saving model to Donna Files/text_512_lstm_gen_weights-improvement-501-0.1704.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1704    \n",
      "Epoch 503/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1910Epoch 00502: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1910    \n",
      "Epoch 504/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1896Epoch 00503: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1895    \n",
      "Epoch 505/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1651Epoch 00504: loss improved from 0.17045 to 0.16510, saving model to Donna Files/text_512_lstm_gen_weights-improvement-504-0.1651.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1651    \n",
      "Epoch 506/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1778Epoch 00505: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1780    \n",
      "Epoch 507/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2078Epoch 00506: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2077    \n",
      "Epoch 508/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1753Epoch 00507: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1754    \n",
      "Epoch 509/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2928Epoch 00508: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2928    \n",
      "Epoch 510/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2370Epoch 00509: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2372    \n",
      "Epoch 511/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2059Epoch 00510: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2061    \n",
      "Epoch 512/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1960Epoch 00511: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1960    \n",
      "Epoch 513/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1907Epoch 00512: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1906    \n",
      "Epoch 514/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1894Epoch 00513: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1895    \n",
      "Epoch 515/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1897Epoch 00514: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1896    \n",
      "Epoch 516/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1553Epoch 00515: loss improved from 0.16510 to 0.15544, saving model to Donna Files/text_512_lstm_gen_weights-improvement-515-0.1554.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1554    \n",
      "Epoch 517/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2090Epoch 00516: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2094    \n",
      "Epoch 518/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3883Epoch 00517: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3882    \n",
      "Epoch 519/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2001Epoch 00518: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2000    \n",
      "Epoch 520/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1747Epoch 00519: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1747    \n",
      "Epoch 521/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1702Epoch 00520: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1705    \n",
      "Epoch 522/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2270Epoch 00521: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2269    \n",
      "Epoch 523/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1678Epoch 00522: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1677    \n",
      "Epoch 524/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1750Epoch 00523: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1750    \n",
      "Epoch 525/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1595Epoch 00524: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1595    \n",
      "Epoch 526/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1595Epoch 00525: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1594    \n",
      "Epoch 527/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00526: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1554    \n",
      "Epoch 528/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1511Epoch 00527: loss improved from 0.15544 to 0.15134, saving model to Donna Files/text_512_lstm_gen_weights-improvement-527-0.1513.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1513    \n",
      "Epoch 529/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2588Epoch 00528: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2587    \n",
      "Epoch 530/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1624Epoch 00529: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1623    \n",
      "Epoch 531/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1424Epoch 00530: loss improved from 0.15134 to 0.14261, saving model to Donna Files/text_512_lstm_gen_weights-improvement-530-0.1426.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1426    \n",
      "Epoch 532/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1456Epoch 00531: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1457    \n",
      "Epoch 533/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1515Epoch 00532: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1514    \n",
      "Epoch 534/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1525Epoch 00533: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1524    \n",
      "Epoch 535/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1596Epoch 00534: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1596    \n",
      "Epoch 536/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1427Epoch 00535: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1427    \n",
      "Epoch 537/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1464Epoch 00536: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1463    \n",
      "Epoch 538/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1434Epoch 00537: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1435    \n",
      "Epoch 539/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1799Epoch 00538: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1799    \n",
      "Epoch 540/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1517Epoch 00539: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1517    \n",
      "Epoch 541/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1806Epoch 00540: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1805    \n",
      "Epoch 542/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1492Epoch 00541: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1493    \n",
      "Epoch 543/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2344Epoch 00542: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2343    \n",
      "Epoch 544/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1753Epoch 00543: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1753    \n",
      "Epoch 545/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1595Epoch 00544: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1598    \n",
      "Epoch 546/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2364Epoch 00545: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2369    \n",
      "Epoch 547/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2388Epoch 00546: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2388    \n",
      "Epoch 548/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1687Epoch 00547: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1686    \n",
      "Epoch 549/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1393Epoch 00548: loss improved from 0.14261 to 0.13931, saving model to Donna Files/text_512_lstm_gen_weights-improvement-548-0.1393.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1393    \n",
      "Epoch 550/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1659Epoch 00549: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1658    \n",
      "Epoch 551/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1294Epoch 00550: loss improved from 0.13931 to 0.12937, saving model to Donna Files/text_512_lstm_gen_weights-improvement-550-0.1294.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1294    \n",
      "Epoch 552/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1298Epoch 00551: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1297    \n",
      "Epoch 553/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1510Epoch 00552: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1516    \n",
      "Epoch 554/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.4983Epoch 00553: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.4980    \n",
      "Epoch 555/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1905Epoch 00554: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1907    \n",
      "Epoch 556/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1937Epoch 00555: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1937    \n",
      "Epoch 557/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1494Epoch 00556: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1494    \n",
      "Epoch 558/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1404Epoch 00557: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1404    \n",
      "Epoch 559/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1221Epoch 00558: loss improved from 0.12937 to 0.12208, saving model to Donna Files/text_512_lstm_gen_weights-improvement-558-0.1221.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.1221    \n",
      "Epoch 560/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1322Epoch 00559: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1321    \n",
      "Epoch 561/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1378Epoch 00560: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1379    \n",
      "Epoch 562/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2946Epoch 00561: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2947    \n",
      "Epoch 563/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2116Epoch 00562: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2115    \n",
      "Epoch 564/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1606Epoch 00563: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1609    \n",
      "Epoch 565/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3548Epoch 00564: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3547    \n",
      "Epoch 566/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1670Epoch 00565: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1669    \n",
      "Epoch 567/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1209Epoch 00566: loss improved from 0.12208 to 0.12092, saving model to Donna Files/text_512_lstm_gen_weights-improvement-566-0.1209.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1209    \n",
      "Epoch 568/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1196Epoch 00567: loss improved from 0.12092 to 0.11966, saving model to Donna Files/text_512_lstm_gen_weights-improvement-567-0.1197.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 0.1197    \n",
      "Epoch 569/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1621Epoch 00568: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1620    \n",
      "Epoch 570/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1198Epoch 00569: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1198    \n",
      "Epoch 571/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1122Epoch 00570: loss improved from 0.11966 to 0.11248, saving model to Donna Files/text_512_lstm_gen_weights-improvement-570-0.1125.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1125    \n",
      "Epoch 572/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1555Epoch 00571: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1554    \n",
      "Epoch 573/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1263Epoch 00572: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1262    \n",
      "Epoch 574/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1225Epoch 00573: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1225    \n",
      "Epoch 575/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1467Epoch 00574: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1466    \n",
      "Epoch 576/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1307Epoch 00575: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1307    \n",
      "Epoch 577/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1332Epoch 00576: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1331    \n",
      "Epoch 578/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1197Epoch 00577: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1197    \n",
      "Epoch 579/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1164Epoch 00578: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1164    \n",
      "Epoch 580/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1175Epoch 00579: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1174    \n",
      "Epoch 581/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1252Epoch 00580: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1253    \n",
      "Epoch 582/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2305Epoch 00581: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2304    \n",
      "Epoch 583/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1560Epoch 00582: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1562    \n",
      "Epoch 584/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2002Epoch 00583: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2003    \n",
      "Epoch 585/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1649Epoch 00584: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1648    \n",
      "Epoch 586/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1269Epoch 00585: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1269    \n",
      "Epoch 587/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2080Epoch 00586: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2080    \n",
      "Epoch 588/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1474Epoch 00587: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1473    \n",
      "Epoch 589/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1060Epoch 00588: loss improved from 0.11248 to 0.10614, saving model to Donna Files/text_512_lstm_gen_weights-improvement-588-0.1061.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1061    \n",
      "Epoch 590/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1203Epoch 00589: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1203    \n",
      "Epoch 591/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1213Epoch 00590: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1217    \n",
      "Epoch 592/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1605Epoch 00591: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1605    \n",
      "Epoch 593/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1263Epoch 00592: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1262    \n",
      "Epoch 594/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1220Epoch 00593: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1222    \n",
      "Epoch 595/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2255Epoch 00594: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2256    \n",
      "Epoch 596/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1619Epoch 00595: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1619    \n",
      "Epoch 597/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1452Epoch 00596: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1451    \n",
      "Epoch 598/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1154Epoch 00597: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1154    \n",
      "Epoch 599/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1198Epoch 00598: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1197    \n",
      "Epoch 600/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1074Epoch 00599: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1073    \n",
      "Epoch 601/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1230Epoch 00600: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1230    \n",
      "Epoch 602/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1402Epoch 00601: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1401    \n",
      "Epoch 603/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1440Epoch 00602: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1439    \n",
      "Epoch 604/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1145Epoch 00603: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1145    \n",
      "Epoch 605/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1780Epoch 00604: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1780    \n",
      "Epoch 606/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1251Epoch 00605: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1251    \n",
      "Epoch 607/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1114Epoch 00606: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1114    \n",
      "Epoch 608/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1139Epoch 00607: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1144    \n",
      "Epoch 609/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2272Epoch 00608: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2272    \n",
      "Epoch 610/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1622Epoch 00609: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1623    \n",
      "Epoch 611/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1427Epoch 00610: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1427    \n",
      "Epoch 612/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1180Epoch 00611: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1180    \n",
      "Epoch 613/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1310Epoch 00612: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1310    \n",
      "Epoch 614/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1018Epoch 00613: loss improved from 0.10614 to 0.10173, saving model to Donna Files/text_512_lstm_gen_weights-improvement-613-0.1017.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1017    \n",
      "Epoch 615/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1039Epoch 00614: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1039    \n",
      "Epoch 616/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1015Epoch 00615: loss improved from 0.10173 to 0.10140, saving model to Donna Files/text_512_lstm_gen_weights-improvement-615-0.1014.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.1014    \n",
      "Epoch 617/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1064Epoch 00616: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1065    \n",
      "Epoch 618/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1370Epoch 00617: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1372    \n",
      "Epoch 619/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3033Epoch 00618: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3031    \n",
      "Epoch 620/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1403Epoch 00619: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1402    \n",
      "Epoch 621/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1194Epoch 00620: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1197    \n",
      "Epoch 622/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2827Epoch 00621: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2827    \n",
      "Epoch 623/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2522Epoch 00622: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2537    \n",
      "Epoch 624/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1230Epoch 00623: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1229    \n",
      "Epoch 625/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0967Epoch 00624: loss improved from 0.10140 to 0.09665, saving model to Donna Files/text_512_lstm_gen_weights-improvement-624-0.0966.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0966    \n",
      "Epoch 626/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0952Epoch 00625: loss improved from 0.09665 to 0.09517, saving model to Donna Files/text_512_lstm_gen_weights-improvement-625-0.0952.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0952    \n",
      "Epoch 627/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1087Epoch 00626: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1087    \n",
      "Epoch 628/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0946Epoch 00627: loss improved from 0.09517 to 0.09459, saving model to Donna Files/text_512_lstm_gen_weights-improvement-627-0.0946.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0946    \n",
      "Epoch 629/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0984Epoch 00628: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0984    \n",
      "Epoch 630/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0938Epoch 00629: loss improved from 0.09459 to 0.09372, saving model to Donna Files/text_512_lstm_gen_weights-improvement-629-0.0937.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0937    \n",
      "Epoch 631/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0929Epoch 00630: loss improved from 0.09372 to 0.09292, saving model to Donna Files/text_512_lstm_gen_weights-improvement-630-0.0929.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0929    \n",
      "Epoch 632/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0852Epoch 00631: loss improved from 0.09292 to 0.08517, saving model to Donna Files/text_512_lstm_gen_weights-improvement-631-0.0852.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0852    \n",
      "Epoch 633/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0834Epoch 00632: loss improved from 0.08517 to 0.08337, saving model to Donna Files/text_512_lstm_gen_weights-improvement-632-0.0834.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0834    \n",
      "Epoch 634/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0929Epoch 00633: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0930    \n",
      "Epoch 635/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1629Epoch 00634: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1630    \n",
      "Epoch 636/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1789Epoch 00635: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1788    \n",
      "Epoch 637/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1159Epoch 00636: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1159    \n",
      "Epoch 638/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1337Epoch 00637: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1336    \n",
      "Epoch 639/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0955Epoch 00638: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0955    \n",
      "Epoch 640/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1045Epoch 00639: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1046    \n",
      "Epoch 641/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1581Epoch 00640: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1581    \n",
      "Epoch 642/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1227Epoch 00641: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1226    \n",
      "Epoch 643/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0996Epoch 00642: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0997    \n",
      "Epoch 644/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1183Epoch 00643: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1182    \n",
      "Epoch 645/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1009Epoch 00644: loss did not improve\n",
      "9990/9990 [==============================] - 30s - loss: 0.1010    \n",
      "Epoch 646/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2049Epoch 00645: loss did not improve\n",
      "9990/9990 [==============================] - 31s - loss: 0.2048    \n",
      "Epoch 647/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1025Epoch 00646: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1024    \n",
      "Epoch 648/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1162Epoch 00647: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1163    \n",
      "Epoch 649/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1415Epoch 00648: loss did not improve\n",
      "9990/9990 [==============================] - 30s - loss: 0.1414    \n",
      "Epoch 650/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0936Epoch 00649: loss did not improve\n",
      "9990/9990 [==============================] - 30s - loss: 0.0936    \n",
      "Epoch 651/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1469Epoch 00650: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1469    \n",
      "Epoch 652/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1248Epoch 00651: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1248    \n",
      "Epoch 653/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0907Epoch 00652: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0906    \n",
      "Epoch 654/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1041Epoch 00653: loss did not improve\n",
      "9990/9990 [==============================] - 31s - loss: 0.1042    \n",
      "Epoch 655/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1062Epoch 00654: loss did not improve\n",
      "9990/9990 [==============================] - 33s - loss: 0.1062    \n",
      "Epoch 656/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1085Epoch 00655: loss did not improve\n",
      "9990/9990 [==============================] - 30s - loss: 0.1085    \n",
      "Epoch 657/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1278Epoch 00656: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1278    \n",
      "Epoch 658/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0984Epoch 00657: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0986    \n",
      "Epoch 659/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1068Epoch 00658: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1070    \n",
      "Epoch 660/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1355Epoch 00659: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1357    \n",
      "Epoch 661/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1251Epoch 00660: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1253    \n",
      "Epoch 662/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3985Epoch 00661: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3983    \n",
      "Epoch 663/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1201Epoch 00662: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.1201    \n",
      "Epoch 664/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0930Epoch 00663: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0930    \n",
      "Epoch 665/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0731Epoch 00664: loss improved from 0.08337 to 0.07313, saving model to Donna Files/text_512_lstm_gen_weights-improvement-664-0.0731.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 0.0731    \n",
      "Epoch 666/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0903Epoch 00665: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0902    \n",
      "Epoch 667/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0867Epoch 00666: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0866    \n",
      "Epoch 668/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0882Epoch 00667: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0882    \n",
      "Epoch 669/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0838Epoch 00668: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0837    \n",
      "Epoch 670/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0836Epoch 00669: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0836    \n",
      "Epoch 671/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1006Epoch 00670: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1010    \n",
      "Epoch 672/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3488Epoch 00671: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3488    \n",
      "Epoch 673/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1459Epoch 00672: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1458    \n",
      "Epoch 674/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1011Epoch 00673: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1011    \n",
      "Epoch 675/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0808Epoch 00674: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0808    \n",
      "Epoch 676/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0798Epoch 00675: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0798    \n",
      "Epoch 677/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0820Epoch 00676: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0820    \n",
      "Epoch 678/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0868Epoch 00677: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0867    \n",
      "Epoch 679/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0702Epoch 00678: loss improved from 0.07313 to 0.07013, saving model to Donna Files/text_512_lstm_gen_weights-improvement-678-0.0701.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 0.0701    \n",
      "Epoch 680/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0588Epoch 00679: loss improved from 0.07013 to 0.05876, saving model to Donna Files/text_512_lstm_gen_weights-improvement-679-0.0588.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 0.0588    \n",
      "Epoch 681/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0739Epoch 00680: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0739    \n",
      "Epoch 682/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0863Epoch 00681: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0862    \n",
      "Epoch 683/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1009Epoch 00682: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1010    \n",
      "Epoch 684/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1166Epoch 00683: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1166    \n",
      "Epoch 685/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0885Epoch 00684: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0885    \n",
      "Epoch 686/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0797Epoch 00685: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0797    \n",
      "Epoch 687/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0898Epoch 00686: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0897    \n",
      "Epoch 688/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0923Epoch 00687: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0939    \n",
      "Epoch 689/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0959Epoch 00688: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0962    \n",
      "Epoch 690/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2150Epoch 00689: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2150    \n",
      "Epoch 691/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1133Epoch 00690: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1133    \n",
      "Epoch 692/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1213Epoch 00691: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1213    \n",
      "Epoch 693/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0866Epoch 00692: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0866    \n",
      "Epoch 694/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0833Epoch 00693: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0835    \n",
      "Epoch 695/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0997Epoch 00694: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0998    \n",
      "Epoch 696/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0994Epoch 00695: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0993    \n",
      "Epoch 697/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0992Epoch 00696: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0993    \n",
      "Epoch 698/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1037Epoch 00697: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1037    \n",
      "Epoch 699/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1061Epoch 00698: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1060    \n",
      "Epoch 700/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1010Epoch 00699: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1010    \n",
      "Epoch 701/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1010Epoch 00700: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1009    \n",
      "Epoch 702/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0806Epoch 00701: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0805    \n",
      "Epoch 703/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0810Epoch 00702: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0809    \n",
      "Epoch 704/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0758Epoch 00703: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0760    \n",
      "Epoch 705/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1418Epoch 00704: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1418    \n",
      "Epoch 706/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1592Epoch 00705: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1591    \n",
      "Epoch 707/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0801Epoch 00706: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0801    \n",
      "Epoch 708/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0789Epoch 00707: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0789    \n",
      "Epoch 709/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0943Epoch 00708: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0943    \n",
      "Epoch 710/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0837Epoch 00709: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0837    \n",
      "Epoch 711/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0715Epoch 00710: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0715    \n",
      "Epoch 712/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0821Epoch 00711: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0821    \n",
      "Epoch 713/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0963Epoch 00712: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0963    \n",
      "Epoch 714/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0831Epoch 00713: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0830    \n",
      "Epoch 715/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0823Epoch 00714: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0823    \n",
      "Epoch 716/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0971Epoch 00715: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0970    \n",
      "Epoch 717/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0925Epoch 00716: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0925    \n",
      "Epoch 718/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0988Epoch 00717: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0988    \n",
      "Epoch 719/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0894Epoch 00718: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0894    \n",
      "Epoch 720/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1338Epoch 00719: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1337    \n",
      "Epoch 721/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1014Epoch 00720: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1013    \n",
      "Epoch 722/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0763Epoch 00721: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0762    \n",
      "Epoch 723/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0614Epoch 00722: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0613    \n",
      "Epoch 724/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0645Epoch 00723: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0645    \n",
      "Epoch 725/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0856Epoch 00724: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0856    \n",
      "Epoch 726/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0901Epoch 00725: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0901    \n",
      "Epoch 727/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0784Epoch 00726: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0785    \n",
      "Epoch 728/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1695Epoch 00727: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1694    \n",
      "Epoch 729/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0884Epoch 00728: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0884    \n",
      "Epoch 730/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0929Epoch 00729: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0928    \n",
      "Epoch 731/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0776Epoch 00730: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0775    \n",
      "Epoch 732/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0837Epoch 00731: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0836    \n",
      "Epoch 733/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0799Epoch 00732: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0799    \n",
      "Epoch 734/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0730Epoch 00733: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0730    \n",
      "Epoch 735/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0954Epoch 00734: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0954    \n",
      "Epoch 736/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0768Epoch 00735: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0768    \n",
      "Epoch 737/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0790Epoch 00736: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0790    \n",
      "Epoch 738/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0699Epoch 00737: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0699    \n",
      "Epoch 739/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0835Epoch 00738: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0834    \n",
      "Epoch 740/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1047Epoch 00739: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1046    \n",
      "Epoch 741/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0996Epoch 00740: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0996    \n",
      "Epoch 742/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1107Epoch 00741: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1106    \n",
      "Epoch 743/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0825Epoch 00742: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0825    \n",
      "Epoch 744/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0853Epoch 00743: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0852    \n",
      "Epoch 745/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0848Epoch 00744: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0850    \n",
      "Epoch 746/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1413Epoch 00745: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1412    \n",
      "Epoch 747/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0811Epoch 00746: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0811    \n",
      "Epoch 748/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0741Epoch 00747: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0741    \n",
      "Epoch 749/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1683Epoch 00748: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1684    \n",
      "Epoch 750/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0930Epoch 00749: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0929    \n",
      "Epoch 751/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0729Epoch 00750: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0729    \n",
      "Epoch 752/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0719Epoch 00751: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0719    \n",
      "Epoch 753/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0704Epoch 00752: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0704    \n",
      "Epoch 754/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0638Epoch 00753: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0638    \n",
      "Epoch 755/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0803Epoch 00754: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0803    \n",
      "Epoch 756/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0980Epoch 00755: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0984    \n",
      "Epoch 757/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3552Epoch 00756: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3550    \n",
      "Epoch 758/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1637Epoch 00757: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1636    \n",
      "Epoch 759/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0910Epoch 00758: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0910    \n",
      "Epoch 760/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0765Epoch 00759: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0765    \n",
      "Epoch 761/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0641Epoch 00760: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0641    \n",
      "Epoch 762/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0554Epoch 00761: loss improved from 0.05876 to 0.05553, saving model to Donna Files/text_512_lstm_gen_weights-improvement-761-0.0555.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0555    \n",
      "Epoch 763/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0896Epoch 00762: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0896    \n",
      "Epoch 764/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0684Epoch 00763: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0684    \n",
      "Epoch 765/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0656Epoch 00764: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0655    \n",
      "Epoch 766/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0727Epoch 00765: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0727    \n",
      "Epoch 767/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0673Epoch 00766: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0673    \n",
      "Epoch 768/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0493Epoch 00767: loss improved from 0.05553 to 0.04925, saving model to Donna Files/text_512_lstm_gen_weights-improvement-767-0.0493.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0493    \n",
      "Epoch 769/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0552Epoch 00768: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0552    \n",
      "Epoch 770/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0537Epoch 00769: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0536    \n",
      "Epoch 771/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0667Epoch 00770: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.0667    \n",
      "Epoch 772/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0704Epoch 00771: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0704    \n",
      "Epoch 773/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0850Epoch 00772: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0850    \n",
      "Epoch 774/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0770Epoch 00773: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0773    \n",
      "Epoch 775/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1834Epoch 00774: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1833    \n",
      "Epoch 776/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0817Epoch 00775: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0817    \n",
      "Epoch 777/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0775Epoch 00776: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0774    \n",
      "Epoch 778/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0651Epoch 00777: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0650    \n",
      "Epoch 779/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0691Epoch 00778: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0690    \n",
      "Epoch 780/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0682Epoch 00779: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0681    \n",
      "Epoch 781/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0801Epoch 00780: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0801    \n",
      "Epoch 782/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0787Epoch 00781: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0786    \n",
      "Epoch 783/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0674Epoch 00782: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0674    \n",
      "Epoch 784/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0827Epoch 00783: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0827    \n",
      "Epoch 785/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0765Epoch 00784: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0765    \n",
      "Epoch 786/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0858Epoch 00785: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0858    \n",
      "Epoch 787/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0915Epoch 00786: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0915    \n",
      "Epoch 788/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0929Epoch 00787: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0928    \n",
      "Epoch 789/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1034Epoch 00788: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1035    \n",
      "Epoch 790/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1088Epoch 00789: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1088    \n",
      "Epoch 791/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0647Epoch 00790: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0647    \n",
      "Epoch 792/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0849Epoch 00791: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0848    \n",
      "Epoch 793/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0775Epoch 00792: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0775    \n",
      "Epoch 794/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0695Epoch 00793: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0696    \n",
      "Epoch 795/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0930Epoch 00794: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0930    \n",
      "Epoch 796/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0957Epoch 00795: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0956    \n",
      "Epoch 797/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1025Epoch 00796: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1024    \n",
      "Epoch 798/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0670Epoch 00797: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0670    \n",
      "Epoch 799/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0654Epoch 00798: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0654    \n",
      "Epoch 800/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0595Epoch 00799: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0595    \n",
      "Epoch 801/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0579Epoch 00800: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0579    \n",
      "Epoch 802/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0641Epoch 00801: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0641    \n",
      "Epoch 803/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0806Epoch 00802: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0807    \n",
      "Epoch 804/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1042Epoch 00803: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1044    \n",
      "Epoch 805/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1357Epoch 00804: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1356    \n",
      "Epoch 806/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0899Epoch 00805: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0898    \n",
      "Epoch 807/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0757Epoch 00806: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0757    \n",
      "Epoch 808/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0724Epoch 00807: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0724    \n",
      "Epoch 809/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0836Epoch 00808: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0836    \n",
      "Epoch 810/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0644Epoch 00809: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0644    \n",
      "Epoch 811/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0863Epoch 00810: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0863    \n",
      "Epoch 812/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0664Epoch 00811: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0677    \n",
      "Epoch 813/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.3727Epoch 00812: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.3726    \n",
      "Epoch 814/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0801Epoch 00813: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0801    \n",
      "Epoch 815/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0610Epoch 00814: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0610    \n",
      "Epoch 816/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0493Epoch 00815: loss improved from 0.04925 to 0.04923, saving model to Donna Files/text_512_lstm_gen_weights-improvement-815-0.0492.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0492    \n",
      "Epoch 817/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0577Epoch 00816: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0577    \n",
      "Epoch 818/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0465Epoch 00817: loss improved from 0.04923 to 0.04644, saving model to Donna Files/text_512_lstm_gen_weights-improvement-817-0.0464.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0464    \n",
      "Epoch 819/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0407Epoch 00818: loss improved from 0.04644 to 0.04071, saving model to Donna Files/text_512_lstm_gen_weights-improvement-818-0.0407.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0407    \n",
      "Epoch 820/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0530Epoch 00819: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0530    \n",
      "Epoch 821/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0525Epoch 00820: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0524    \n",
      "Epoch 822/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0594Epoch 00821: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0593    \n",
      "Epoch 823/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0552Epoch 00822: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0552    \n",
      "Epoch 824/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0674Epoch 00823: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0674    \n",
      "Epoch 825/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2994Epoch 00824: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2992    \n",
      "Epoch 826/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0858Epoch 00825: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0858    \n",
      "Epoch 827/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0595Epoch 00826: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0595    \n",
      "Epoch 828/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0623Epoch 00827: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0622    \n",
      "Epoch 829/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0577Epoch 00828: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0577    \n",
      "Epoch 830/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0635Epoch 00829: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0635    \n",
      "Epoch 831/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0616Epoch 00830: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0616    \n",
      "Epoch 832/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0562Epoch 00831: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0562    \n",
      "Epoch 833/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0465Epoch 00832: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0465    \n",
      "Epoch 834/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0444Epoch 00833: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0444    \n",
      "Epoch 835/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0483Epoch 00834: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0482    \n",
      "Epoch 836/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0574Epoch 00835: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0574    \n",
      "Epoch 837/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1012Epoch 00836: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1012    \n",
      "Epoch 838/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1009Epoch 00837: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1009    \n",
      "Epoch 839/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0708Epoch 00838: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0718    \n",
      "Epoch 840/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2261Epoch 00839: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2259    \n",
      "Epoch 841/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0924Epoch 00840: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0924    \n",
      "Epoch 842/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0621Epoch 00841: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0621    \n",
      "Epoch 843/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0565Epoch 00842: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0565    \n",
      "Epoch 844/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0607Epoch 00843: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0607    \n",
      "Epoch 845/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0540Epoch 00844: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0545    \n",
      "Epoch 846/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1655Epoch 00845: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1654    \n",
      "Epoch 847/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0743Epoch 00846: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0743    \n",
      "Epoch 848/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0739Epoch 00847: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0739    \n",
      "Epoch 849/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0647Epoch 00848: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0646    \n",
      "Epoch 850/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0525Epoch 00849: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0524    \n",
      "Epoch 851/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0532Epoch 00850: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0533    \n",
      "Epoch 852/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0540Epoch 00851: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0540    \n",
      "Epoch 853/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0464Epoch 00852: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0464    \n",
      "Epoch 854/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0576Epoch 00853: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0576    \n",
      "Epoch 855/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0672Epoch 00854: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0672    \n",
      "Epoch 856/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0713Epoch 00855: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0718    \n",
      "Epoch 857/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1072Epoch 00856: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1073    \n",
      "Epoch 858/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2219Epoch 00857: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.2218    \n",
      "Epoch 859/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0659Epoch 00858: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0659    \n",
      "Epoch 860/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0502Epoch 00859: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0502    \n",
      "Epoch 861/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0499Epoch 00860: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0499    \n",
      "Epoch 862/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0479Epoch 00861: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0479    \n",
      "Epoch 863/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0665Epoch 00862: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0665    \n",
      "Epoch 864/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0549Epoch 00863: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0550    \n",
      "Epoch 865/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0532Epoch 00864: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0532    \n",
      "Epoch 866/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0477Epoch 00865: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0477    \n",
      "Epoch 867/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0562Epoch 00866: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0563    \n",
      "Epoch 868/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1105Epoch 00867: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1104    \n",
      "Epoch 869/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0631Epoch 00868: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0630    \n",
      "Epoch 870/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0704Epoch 00869: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0704    \n",
      "Epoch 871/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0572Epoch 00870: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0572    \n",
      "Epoch 872/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0587Epoch 00871: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0587    \n",
      "Epoch 873/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0521Epoch 00872: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0521    \n",
      "Epoch 874/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0682Epoch 00873: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0682    \n",
      "Epoch 875/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0761Epoch 00874: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0761    \n",
      "Epoch 876/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0561Epoch 00875: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0561    \n",
      "Epoch 877/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0594Epoch 00876: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0593    \n",
      "Epoch 878/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0474Epoch 00877: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0474    \n",
      "Epoch 879/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0596Epoch 00878: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0596    \n",
      "Epoch 880/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1215Epoch 00879: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1215    \n",
      "Epoch 881/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0860Epoch 00880: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0860    \n",
      "Epoch 882/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0575Epoch 00881: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0575    \n",
      "Epoch 883/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0675Epoch 00882: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0674    \n",
      "Epoch 884/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0613Epoch 00883: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0613    \n",
      "Epoch 885/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0522Epoch 00884: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0522    \n",
      "Epoch 886/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0734Epoch 00885: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0733    \n",
      "Epoch 887/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0601Epoch 00886: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0601    \n",
      "Epoch 888/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0710Epoch 00887: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0710    \n",
      "Epoch 889/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0960Epoch 00888: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0960    \n",
      "Epoch 890/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0795Epoch 00889: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0795    \n",
      "Epoch 891/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0615Epoch 00890: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0615    \n",
      "Epoch 892/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0654Epoch 00891: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0654    \n",
      "Epoch 893/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0626Epoch 00892: loss did not improve\n",
      "9990/9990 [==============================] - 29s - loss: 0.0626    \n",
      "Epoch 894/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1012Epoch 00893: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1013    \n",
      "Epoch 895/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1216Epoch 00894: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1215    \n",
      "Epoch 896/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0583Epoch 00895: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0583    \n",
      "Epoch 897/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0465Epoch 00896: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0465    \n",
      "Epoch 898/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0470Epoch 00897: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0470    \n",
      "Epoch 899/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0479Epoch 00898: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0479    \n",
      "Epoch 900/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0486Epoch 00899: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0486    \n",
      "Epoch 901/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0472Epoch 00900: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0471    \n",
      "Epoch 902/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0462Epoch 00901: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0462    \n",
      "Epoch 903/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0506Epoch 00902: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0506    \n",
      "Epoch 904/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0559Epoch 00903: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0559    \n",
      "Epoch 905/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0582Epoch 00904: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0581    \n",
      "Epoch 906/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1029Epoch 00905: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1029    \n",
      "Epoch 907/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0697Epoch 00906: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0697    \n",
      "Epoch 908/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0691Epoch 00907: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0691    \n",
      "Epoch 909/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0669Epoch 00908: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0668    \n",
      "Epoch 910/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0592Epoch 00909: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0592    \n",
      "Epoch 911/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0610Epoch 00910: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0609    \n",
      "Epoch 912/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0510Epoch 00911: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0510    \n",
      "Epoch 913/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0651Epoch 00912: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0651    \n",
      "Epoch 914/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0837Epoch 00913: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0836    \n",
      "Epoch 915/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0607Epoch 00914: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0607    \n",
      "Epoch 916/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0654Epoch 00915: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0654    \n",
      "Epoch 917/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0608Epoch 00916: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0607    \n",
      "Epoch 918/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0606Epoch 00917: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0607    \n",
      "Epoch 919/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0935Epoch 00918: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0935    \n",
      "Epoch 920/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0709Epoch 00919: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0709    \n",
      "Epoch 921/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0470Epoch 00920: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0470    \n",
      "Epoch 922/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0471Epoch 00921: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0471    \n",
      "Epoch 923/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0514Epoch 00922: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0514    \n",
      "Epoch 924/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0447Epoch 00923: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0448    \n",
      "Epoch 925/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0544Epoch 00924: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0544    \n",
      "Epoch 926/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0456Epoch 00925: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0456    \n",
      "Epoch 927/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0568Epoch 00926: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0568    \n",
      "Epoch 928/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1349Epoch 00927: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1349    \n",
      "Epoch 929/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0897Epoch 00928: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0897    \n",
      "Epoch 930/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0665Epoch 00929: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0665    \n",
      "Epoch 931/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0541Epoch 00930: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0541    \n",
      "Epoch 932/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0379Epoch 00931: loss improved from 0.04071 to 0.03784, saving model to Donna Files/text_512_lstm_gen_weights-improvement-931-0.0378.hdf5\n",
      "9990/9990 [==============================] - 29s - loss: 0.0378    \n",
      "Epoch 933/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0367Epoch 00932: loss improved from 0.03784 to 0.03673, saving model to Donna Files/text_512_lstm_gen_weights-improvement-932-0.0367.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0367    \n",
      "Epoch 934/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0532Epoch 00933: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0532    \n",
      "Epoch 935/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0697Epoch 00934: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0697    \n",
      "Epoch 936/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0375Epoch 00935: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0375    \n",
      "Epoch 937/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0431Epoch 00936: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0431    \n",
      "Epoch 938/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0504Epoch 00937: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0504    \n",
      "Epoch 939/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0587Epoch 00938: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0587    \n",
      "Epoch 940/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0437Epoch 00939: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0437    \n",
      "Epoch 941/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0684Epoch 00940: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0685    \n",
      "Epoch 942/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1013Epoch 00941: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1012    \n",
      "Epoch 943/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0680Epoch 00942: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0680    \n",
      "Epoch 944/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0638Epoch 00943: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0638    \n",
      "Epoch 945/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0489Epoch 00944: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0488    \n",
      "Epoch 946/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0402Epoch 00945: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.0401    \n",
      "Epoch 947/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0392Epoch 00946: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0394    \n",
      "Epoch 948/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1254Epoch 00947: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1254    \n",
      "Epoch 949/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0974Epoch 00948: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0974    \n",
      "Epoch 950/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0552Epoch 00949: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0552    \n",
      "Epoch 951/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0517Epoch 00950: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0518    \n",
      "Epoch 952/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0901Epoch 00951: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0900    \n",
      "Epoch 953/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0500Epoch 00952: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0500    \n",
      "Epoch 954/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0639Epoch 00953: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0639    \n",
      "Epoch 955/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0593Epoch 00954: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0593    \n",
      "Epoch 956/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0414Epoch 00955: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0413    \n",
      "Epoch 957/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0529Epoch 00956: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0528    \n",
      "Epoch 958/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0401Epoch 00957: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0400    \n",
      "Epoch 959/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0502Epoch 00958: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0502    \n",
      "Epoch 960/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0543Epoch 00959: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0542    \n",
      "Epoch 961/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0486Epoch 00960: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0486    \n",
      "Epoch 962/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0611Epoch 00961: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0611    \n",
      "Epoch 963/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0485Epoch 00962: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0487    \n",
      "Epoch 964/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1665Epoch 00963: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1666    \n",
      "Epoch 965/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.2379Epoch 00964: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.2378    \n",
      "Epoch 966/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0680Epoch 00965: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0680    \n",
      "Epoch 967/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0487Epoch 00966: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0486    \n",
      "Epoch 968/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0431Epoch 00967: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0431    \n",
      "Epoch 969/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0395Epoch 00968: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0395    \n",
      "Epoch 970/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0264Epoch 00969: loss improved from 0.03673 to 0.02640, saving model to Donna Files/text_512_lstm_gen_weights-improvement-969-0.0264.hdf5\n",
      "9990/9990 [==============================] - 28s - loss: 0.0264    \n",
      "Epoch 971/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0295Epoch 00970: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0295    \n",
      "Epoch 972/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0379Epoch 00971: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0379    \n",
      "Epoch 973/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0406Epoch 00972: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0406    \n",
      "Epoch 974/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0599Epoch 00973: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0599    \n",
      "Epoch 975/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0491Epoch 00974: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0491    \n",
      "Epoch 976/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0368Epoch 00975: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0368    \n",
      "Epoch 977/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0368Epoch 00976: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0368    \n",
      "Epoch 978/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0379Epoch 00977: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0379    \n",
      "Epoch 979/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0477Epoch 00978: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0479    \n",
      "Epoch 980/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1374Epoch 00979: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.1374    \n",
      "Epoch 981/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0616Epoch 00980: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0616    \n",
      "Epoch 982/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0524Epoch 00981: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0523    \n",
      "Epoch 983/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0452Epoch 00982: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0452    \n",
      "Epoch 984/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0446Epoch 00983: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0446    \n",
      "Epoch 985/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0468Epoch 00984: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0467    \n",
      "Epoch 986/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0441Epoch 00985: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0440    \n",
      "Epoch 987/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0474Epoch 00986: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0474    \n",
      "Epoch 988/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0617Epoch 00987: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0617    \n",
      "Epoch 989/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0705Epoch 00988: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0705    \n",
      "Epoch 990/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.1285Epoch 00989: loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9990/9990 [==============================] - 28s - loss: 0.1284    \n",
      "Epoch 991/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0737Epoch 00990: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0737    \n",
      "Epoch 992/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0435Epoch 00991: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0435    \n",
      "Epoch 993/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0480Epoch 00992: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0480    \n",
      "Epoch 994/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0467Epoch 00993: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0466    \n",
      "Epoch 995/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0553Epoch 00994: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0553    \n",
      "Epoch 996/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0382Epoch 00995: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0382    \n",
      "Epoch 997/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0390Epoch 00996: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0390    \n",
      "Epoch 998/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0400Epoch 00997: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0400    \n",
      "Epoch 999/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0346Epoch 00998: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0346    \n",
      "Epoch 1000/1000\n",
      "9984/9990 [============================>.] - ETA: 0s - loss: 0.0361Epoch 00999: loss did not improve\n",
      "9990/9990 [==============================] - 28s - loss: 0.0360    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f716647a0d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Donna Files/text_512_lstm_gen_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=1000, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "model=load_model(\"Donna Files/text_generator_model.h5\")\n",
    "\n",
    "filename = \"Donna Files/text_512_lstm_gen_weights-improvement-969-0.0264.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" Hello . How are you . Where are you . You Suck \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "sentence=\"The weather is so nice today\"\n",
    "#pattern=sentence.split()\n",
    "pattern = [\"Hello\",\".\",\"How\", \"are\", \"you\",\".\",\"Where\",\"are\", \"you\", \".\",\"You\", \"Suck\"]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ' '.join([str(value) for value in pattern]), \"\\\"\"\n",
    "indexed_input=[]\n",
    "([indexed_input.append(bag_of_words[value.lower()]) for value in pattern])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern=indexed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pattern+=list(np.zeros(int(math.ceil(len(pattern)/float(seq_length))*seq_length)-len(pattern)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pattern)/float(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(len(pattern)/float(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4795,\n",
       " 40,\n",
       " 17,\n",
       " 190,\n",
       " 22,\n",
       " 40,\n",
       " 200,\n",
       " 190,\n",
       " 22,\n",
       " 40,\n",
       " 22,\n",
       " 13087,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110657"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was uppity mean. go you tokyo.. ...right by i'm...god ...right \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(10):\n",
    "\tx = np.reshape(pattern, (len(pattern)/seq_length, seq_length, 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = np.argmax(prediction)\n",
    "\tresult = index_to_words[index]\n",
    "\t#seq_in = [index_to_words[value] for value in pattern]\n",
    "\tsys.stdout.write(result+\" \")\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcpu",
   "language": "python",
   "name": "tfcpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

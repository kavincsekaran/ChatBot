{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_emotion_df=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_emotion_df=pd.read_csv(\"DataSets/text_emotion.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_emotions=pd.Series(tweet_emotion_df[\"sentiment\"]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_emotions={}\n",
    "index_to_emotions={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anger': 13,\n",
       " 'boredom': 11,\n",
       " 'empty': 1,\n",
       " 'enthusiasm': 3,\n",
       " 'fun': 8,\n",
       " 'happiness': 10,\n",
       " 'hate': 9,\n",
       " 'love': 7,\n",
       " 'neutral': 4,\n",
       " 'relief': 12,\n",
       " 'sadness': 2,\n",
       " 'surprise': 6,\n",
       " 'worry': 5}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('bag_of_emotions.json', 'w') as fp:\n",
    "    json.dump(bag_of_emotions, fp)\n",
    "with open('index_to_emotions.json', 'w') as fp:\n",
    "    json.dump(index_to_emotions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index=1\n",
    "for emotion in unique_emotions:\n",
    "    bag_of_emotions[emotion]=index\n",
    "    index_to_emotions[index]=emotion\n",
    "    index+=1\n",
    "#n_chars = len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tweet_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in tweet_emotion_df[\"content\"]:\n",
    "    all_tweet_words.append(list(filter(None,re.split(' |\\'|!|:|;|\\?|\"|\\(|\\)|/|\\.+?|&|\\-\\-|\\*|<.*?>|</.*?>', tweet.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(all_tweet_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_tweets=[]\n",
    "all_words=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tweet in all_tweet_words:\n",
    "    tweet_words=[x for x in tweet if not x.startswith(('@','#')) and not x.isdigit()]\n",
    "    cleaned_tweets.append(tweet_words)\n",
    "    all_words+=tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cleaned_tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_unique_words=pd.Series(all_words).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36084\n"
     ]
    }
   ],
   "source": [
    "bag_of_words={}\n",
    "index_to_words={}\n",
    "index=1\n",
    "for word in all_unique_words:\n",
    "    bag_of_words[word]=index\n",
    "    index_to_words[index]=word\n",
    "    index+=1\n",
    "#n_chars = len(tokenized)\n",
    "n_vocab = len(all_unique_words)\n",
    "print(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setNeuralNet(input_neurons, hidden_neurons, output_neurons):\n",
    "    neural_net={}\n",
    "    neural_net[\"hidden\"]=np.random.randn(hidden_neurons,input_neurons)\n",
    "    neural_net[\"output\"]=np.random.randn(output_neurons,hidden_neurons)\n",
    "    return neural_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "def forward_propagation(net, inputs):\n",
    "    hidden=np.dot(net[\"hidden\"], inputs)\n",
    "    hidden[hidden<0] = 0\n",
    "    out=np.dot(net[\"output\"], sigmoid(hidden))\n",
    "    return sigmoid(hidden), sigmoid(out)\n",
    "    \n",
    "def diff(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "def back_propagate(neuron_outputs,y):\n",
    "    out_errors=[]\n",
    "    hidden_errors=[]\n",
    "    hidden_deltas=[0]*len(nnet[\"hidden\"])\n",
    "    for y_hat in neuron_outputs:\n",
    "        out_err=(y-y_hat)*diff(y_hat)\n",
    "        out_errors.append(out_err)\n",
    "        for out_weight in nnet[\"output\"]:\n",
    "            weights=[x * out_err for x in out_weight]\n",
    "            hidden_errors.append([x *diff(y_hat) for x in weights])\n",
    "    for delta in hidden_errors:\n",
    "        hidden_deltas= [sum(x) for x in zip(hidden_deltas, delta)]\n",
    "    return hidden_deltas, out_errors\n",
    "\n",
    "def update_weights(layer, deltas, learning_rate, inputs):\n",
    "    neuron_index=0\n",
    "    new_weights=[]\n",
    "    for neurons in nnet[layer]:\n",
    "        new_neuron_weights=[]\n",
    "        index=0\n",
    "        for weights in neurons:\n",
    "            new_neuron_weights.append(weights+(learning_rate*deltas[neuron_index]*inputs[index]))\n",
    "            index+=1\n",
    "        neuron_index+=1\n",
    "        new_weights.append(new_neuron_weights)\n",
    "    nnet[layer]=new_weights\n",
    "\n",
    "def getAccuracy(y, y_hat):\n",
    "    correct=0\n",
    "    for index in range(1,len(y)):\n",
    "        if y[index]==y_hat[index]:\n",
    "            correct+=1\n",
    "    print(\"Number of instances tested: \"+ str(len(y)))\n",
    "    print(\"Number of instances classified correctly: \"+ str(correct))\n",
    "    return(round(float(correct)/len(y)*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(nnet, df, epochs):\n",
    "    learning_rate=0.5\n",
    "    #epochs=100\n",
    "    #df=pd.read_csv(\"E:/College Work/CS534 - AI/Chandrasekaran_Kavin_Assign4/heart_data.csv\")\n",
    "    is_train=np.random.rand(len(df))<0.8\n",
    "    train, test = df[is_train], df[~is_train]\n",
    "    #del train[\"is_train\"]\n",
    "    #del test[\"is_train\"]\n",
    "    for epoch in range(epochs):\n",
    "        rse=0\n",
    "        for train_instance in train.itertuples():\n",
    "            input_in=train_instance[1:-1]\n",
    "            hidden_out, output_out=forward_propagation(nnet,input_in)\n",
    "            y=train_instance[-1]\n",
    "            rse+=sum((y-output_out)**2)\n",
    "            hidden_deltas, out_deltas=back_propagate(output_out, y)\n",
    "            update_weights(\"hidden\",hidden_deltas,learning_rate, input_in)\n",
    "            update_weights(\"output\",out_deltas,learning_rate, hidden_out)\n",
    "        #print('Epoch= %d, RSE= %.2f' % (epoch, rse))\n",
    "    y_hat=[]\n",
    "    y=[]\n",
    "    for test_instance in test.itertuples():\n",
    "        input_in=test_instance[1:-1]\n",
    "        hidden_out, output_out=forward_propagation(nnet,input_in)\n",
    "        y.append(test_instance[-1])\n",
    "        if(output_out>0.5):\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            y_hat.append(0)\n",
    "    accuracy=getAccuracy(y, y_hat)\n",
    "    print(\"Accuracy for ANN with backpropagation is %: \"+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnet=setNeuralNet(len(all_unique_words), 10000, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main(nnet,tweets_df[:1000],1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_x=pd.DataFrame(columns=list(all_unique_words)+[\"emotion_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_x_cols=list(all_unique_words)+[\"emotion_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tweets_x_cols).to_csv(\"DataSets/colum_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store['tweets_emotion']=tweets_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = store.root.tweets_emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store.put('tweets_emotion',tweets_x, format='table', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_row={}\n",
    "for word in all_unique_words:\n",
    "    empty_row[word]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_row=empty_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions=tweet_emotion_df[\"sentiment\"]\n",
    "y_index=0\n",
    "for tweet in cleaned_tweets[:5]:\n",
    "    tweet_row=empty_row\n",
    "    for word in all_unique_words:\n",
    "        tweet_row[word]+=1\n",
    "    tweet_row[\"emotion_y\"]=bag_of_emotions[emotions[y_index]]\n",
    "    row_df=pd.DataFrame(tweet_row, index=[y_index])\n",
    "    store.put('tweets_emotion',row_df, format='table', append=True, data_columns=tweets_x_cols)\n",
    "    y_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "empty_row=[0]*len(tweets_x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions=tweet_emotion_df[\"sentiment\"]\n",
    "normalized_words=[]\n",
    "y_index=0\n",
    "for tweet in cleaned_tweets:\n",
    "    tweet_row=empty_row\n",
    "    index=0\n",
    "    for word in tweets_x_cols:\n",
    "        if(word in tweet):\n",
    "            tweet_row[index]+=1\n",
    "        index+=1\n",
    "    tweet_row[-1]=bag_of_emotions[emotions[y_index]]\n",
    "    normalized_words.append(tweet_row)\n",
    "    #store.put('tweets_emotion',tweet_row, format='table', append=True, data_columns=tweets_x_cols)\n",
    "    y_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(normalized_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"emotion_matrix.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df=pd.read_csv(\"Donna Files/emo_mat_1.csv\", names=tweets_x_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df=pd.read_csv(\"Donna Files/emo_mat_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.columns=tweets_x_cols[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_x_cols=pd.read_csv(\"DataSets/colum_names.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>listenin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>habit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>earlier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>started</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>freakin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>his</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>=[</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>layin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>bed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>headache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>ughhhh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>waitin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>your</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>funeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>ceremony</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>gloomy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36055</th>\n",
       "      <td>36055</td>\n",
       "      <td>spur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36056</th>\n",
       "      <td>36056</td>\n",
       "      <td>tisk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36057</th>\n",
       "      <td>36057</td>\n",
       "      <td>tisk,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36058</th>\n",
       "      <td>36058</td>\n",
       "      <td>waxing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36059</th>\n",
       "      <td>36059</td>\n",
       "      <td>mustache</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36060</th>\n",
       "      <td>36060</td>\n",
       "      <td>tfl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36061</th>\n",
       "      <td>36061</td>\n",
       "      <td>hws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36062</th>\n",
       "      <td>36062</td>\n",
       "      <td>chan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36063</th>\n",
       "      <td>36063</td>\n",
       "      <td>kmrn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36064</th>\n",
       "      <td>36064</td>\n",
       "      <td>bawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36065</th>\n",
       "      <td>36065</td>\n",
       "      <td>kosong,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36066</th>\n",
       "      <td>36066</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36067</th>\n",
       "      <td>36067</td>\n",
       "      <td>son-in-laws</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36068</th>\n",
       "      <td>36068</td>\n",
       "      <td>policies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36069</th>\n",
       "      <td>36069</td>\n",
       "      <td>feds,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36070</th>\n",
       "      <td>36070</td>\n",
       "      <td>hounding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36071</th>\n",
       "      <td>36071</td>\n",
       "      <td>dodged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36072</th>\n",
       "      <td>36072</td>\n",
       "      <td>dya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36073</th>\n",
       "      <td>36073</td>\n",
       "      <td>gatwick-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36074</th>\n",
       "      <td>36074</td>\n",
       "      <td>frolic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36075</th>\n",
       "      <td>36075</td>\n",
       "      <td>consummate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36076</th>\n",
       "      <td>36076</td>\n",
       "      <td>thorough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36077</th>\n",
       "      <td>36077</td>\n",
       "      <td>degrease</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36078</th>\n",
       "      <td>36078</td>\n",
       "      <td>diaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36079</th>\n",
       "      <td>36079</td>\n",
       "      <td>succesfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36080</th>\n",
       "      <td>36080</td>\n",
       "      <td>ipsohot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36081</th>\n",
       "      <td>36081</td>\n",
       "      <td>sightseeing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36082</th>\n",
       "      <td>36082</td>\n",
       "      <td>gaijin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36083</th>\n",
       "      <td>36083</td>\n",
       "      <td>godzilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36084</th>\n",
       "      <td>36084</td>\n",
       "      <td>emotion_y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36085 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            0\n",
       "0               0            i\n",
       "1               1         know\n",
       "2               2          was\n",
       "3               3     listenin\n",
       "4               4           to\n",
       "5               5          bad\n",
       "6               6        habit\n",
       "7               7      earlier\n",
       "8               8          and\n",
       "9               9      started\n",
       "10             10      freakin\n",
       "11             11           at\n",
       "12             12          his\n",
       "13             13         part\n",
       "14             14           =[\n",
       "15             15        layin\n",
       "16             16            n\n",
       "17             17          bed\n",
       "18             18         with\n",
       "19             19            a\n",
       "20             20     headache\n",
       "21             21       ughhhh\n",
       "22             22       waitin\n",
       "23             23           on\n",
       "24             24         your\n",
       "25             25         call\n",
       "26             26      funeral\n",
       "27             27     ceremony\n",
       "28             28       gloomy\n",
       "29             29       friday\n",
       "...           ...          ...\n",
       "36055       36055         spur\n",
       "36056       36056         tisk\n",
       "36057       36057        tisk,\n",
       "36058       36058       waxing\n",
       "36059       36059     mustache\n",
       "36060       36060          tfl\n",
       "36061       36061          hws\n",
       "36062       36062         chan\n",
       "36063       36063         kmrn\n",
       "36064       36064         bawa\n",
       "36065       36065      kosong,\n",
       "36066       36066       nation\n",
       "36067       36067  son-in-laws\n",
       "36068       36068     policies\n",
       "36069       36069        feds,\n",
       "36070       36070     hounding\n",
       "36071       36071       dodged\n",
       "36072       36072          dya\n",
       "36073       36073     gatwick-\n",
       "36074       36074       frolic\n",
       "36075       36075   consummate\n",
       "36076       36076     thorough\n",
       "36077       36077     degrease\n",
       "36078       36078      diaries\n",
       "36079       36079  succesfully\n",
       "36080       36080      ipsohot\n",
       "36081       36081  sightseeing\n",
       "36082       36082       gaijin\n",
       "36083       36083     godzilla\n",
       "36084       36084    emotion_y\n",
       "\n",
       "[36085 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_x_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_df.ix[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df[\"emotion_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(store[\"tweets_emotion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store=tables.open_file(\"E:/College Work/CS534 - AI/Donna_h5.h5\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import keras.utils\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "store=pd.HDFStore(\"E:/College Work/CS534 - AI/Donna_h5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = tweets_df.ix[:,:-1]\n",
    "Y = tweets_df[\"emotion_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions_y=[]\n",
    "empty_emotion_row=[0]*8\n",
    "for index in Y:\n",
    "    emotion_row=empty_emotion_row\n",
    "    emotion_row[index]=1\n",
    "    emotions_y.append(emotion_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_reshaped=X.as_matrix().reshape(9999,36084)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotions_y=keras.utils.to_categorical(Y,num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(2000, input_dim=X.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(500, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"Donna Files/emotion_classifier_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "filepath=\"Donna Files/emotion_forward_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.fit(X_reshaped, emotions_y, epochs=150, batch_size=100, callbacks=callbacks_list)\n",
    "# calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=load_model(\"Donna Files/emotion_classifier_model.h5\")\n",
    "filename = \"Donna Files/emotion_forward_weights-improvement-00-1.9928.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_line=\"Hello Dumb dumb Screw you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_line=input_line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_line_vector=[0]*(len(tweets_x_cols)-1)\n",
    "for word in input_line:\n",
    "    if(word in list(tweets_x_cols[\"0\"])):\n",
    "        input_line_vector[list(tweets_x_cols[\"0\"]).index(word)]=1\n",
    "if(sum(input_line_vector)>0):\n",
    "    prediction=model.predict(np.reshape(input_line_vector, (1,len(input_line_vector))))\n",
    "else:\n",
    "    prediction=0\n",
    "list(prediction[0]).index(max(prediction[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_line_vector[list(tweets_x_cols[\"0\"]).index(\"you\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fa74e62414d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# round predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrounded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrounded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1573\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1201\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 786\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 994\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    995\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1129\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1130\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1134\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kavin/tfcpu/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1117\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_reshaped)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcpu",
   "language": "python",
   "name": "tfcpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

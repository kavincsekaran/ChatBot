{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LSTM network and generate text\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import h5py\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mov_lines_file=open(\"/home/kavin/Silo/College Work/AI/DataSets/cornell_movie_dialogs_corpus/cornell movie-dialogs corpus/movie_lines.txt\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_lines=mov_lines_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogues=[]\n",
    "for mov_line in mov_lines:\n",
    "    matched=re.match('.* \\+\\+\\+\\$\\+\\+\\+ (.+)', mov_line)\n",
    "    dialogues.append(matched.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They do to!'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dialogues[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dialogues=[]\n",
    "for line in dialogues:\n",
    "    tokenized_dialogues.append(filter(None,re.split(' |!|\\.|:|;|,|\\?|\"|\\-\\-|\\*|<.*?>|</.*?>', line.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'do', 'not']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dialogues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_dialogues=[]\n",
    "for line in tokenized_dialogues:\n",
    "    line_list=[]\n",
    "    for token in line:\n",
    "        try:\n",
    "            matched=re.match('([\\*|\\'|_|\\-|\\[]|)(.*)([\\*|\\'|_|\\-|\\]]|)', token)\n",
    "            line_list.append(matched.group(2))\n",
    "        except:\n",
    "            pass\n",
    "    cleaned_dialogues.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they', 'do', 'not']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dialogues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dialogues=[]\n",
    "for line in cleaned_dialogues:\n",
    "    all_dialogues+=line\n",
    "tokens=pd.Series(all_dialogues).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words={}\n",
    "index_to_words={}\n",
    "index=0\n",
    "for word in tokens:\n",
    "    bag_of_words[word]=index\n",
    "    index_to_words[index]=word\n",
    "    index+=1\n",
    "n_chars = len(tokenized)\n",
    "n_vocab = len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62141"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['they', 'do', 'not'],\n",
       " ['they', 'do', 'to'],\n",
       " ['i', 'hope', 'so'],\n",
       " ['she', 'okay'],\n",
       " [\"let's\", 'go'],\n",
       " ['wow'],\n",
       " ['okay', \"you're\", 'gonna', 'need', 'to', 'learn', 'how', 'to', 'lie'],\n",
       " ['no'],\n",
       " [\"i'm\",\n",
       "  'kidding',\n",
       "  'you',\n",
       "  'know',\n",
       "  'how',\n",
       "  'sometimes',\n",
       "  'you',\n",
       "  'just',\n",
       "  'become',\n",
       "  'this',\n",
       "  'persona',\n",
       "  'and',\n",
       "  'you',\n",
       "  \"don't\",\n",
       "  'know',\n",
       "  'how',\n",
       "  'to',\n",
       "  'quit'],\n",
       " ['like', 'my', 'fear', 'of', 'wearing', 'pastels']]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dialogues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_dialogues=[]\n",
    "for line in cleaned_dialogues:\n",
    "    line_list=[]\n",
    "    for token in line:\n",
    "        try:\n",
    "            line_list.append(bag_of_words[token])\n",
    "        except:\n",
    "            line_list.append('-1')\n",
    "    indexed_dialogues.append(line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2],\n",
       " [0, 1, 3],\n",
       " [4, 5, 6],\n",
       " [7, 8],\n",
       " [9, 10],\n",
       " [11],\n",
       " [8, 12, 13, 14, 3, 15, 16, 3, 17],\n",
       " [18],\n",
       " [19, 20, 21, 22, 16, 23, 21, 24, 25, 26, 27, 28, 21, 29, 22, 16, 3, 30],\n",
       " [31, 32, 33, 34, 35, 36]]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_dialogues[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens=pd.read_csv(\"/home/kavin/Silo/College Work/DS501/Case 5/10000_email_tokens.csv\", names=(\"tokens\",))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cursors_list=collection.find({\"x_folder\":{\"$regex\":\".*INBOX.*\",\"$options\": \"-i\"}}, {\"subject\":1, \"_id\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_subjects=[]\n",
    "unique_subjects=[]\n",
    "for cursor in cursors_list[:10000]:\n",
    "    all_subjects.append(re.sub('.*FW:.*|.*RE:.*|.*fw:.*|.*re:.*|.*Fw:.*|.*Re:.*', '', cursor['subject']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_subjects=pd.Series(all_subjects).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_rec_msgs=[]\n",
    "all_sent_msgs=[]\n",
    "for subject in unique_subjects[:10]:\n",
    "    cursors_list=collection.find({\"x_folder\":{\"$regex\":\".*INBOX.*\",\"$options\": \"-i\"}, \"subject\":subject}, {\"message\":1, \"_id\":0})\n",
    "    for cursor in cursors_list:\n",
    "        all_rec_msgs.append(cursor['message'])\n",
    "    cursors_list=collection.find({\"x_folder\":{\"$regex\":\".*sent.*\",\"$options\": \"-i\"}, \"subject\":subject}, {\"message\":1, \"_id\":0})\n",
    "    for cursor in cursors_list:\n",
    "        all_sent_msgs.append(cursor['message'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokenized=[]\n",
    "for message in all_sent_msgs:\n",
    "    sent_tokenized+=re.split('\\W+|,| |\\*| \\n|\\t|:|_', message)\n",
    "recv_tokenized=[]\n",
    "for message in all_rec_msgs:\n",
    "    recv_tokenized+=re.split('\\W+|,| |\\*| \\n|\\t|:|_', message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_file=open(\"/home/kavin/Silo/College Work/DS501/Case 5/sent_email_tokens.csv\",'w')\n",
    "for token in sent_tokenized:\n",
    "    if len(token) >0:\n",
    "        tokens_file.write(token+\"\\n\")\n",
    "tokens_file.close()\n",
    "tokens_file=open(\"/home/kavin/Silo/College Work/DS501/Case 5/received_email_tokens.csv\",'w')\n",
    "for token in recv_tokenized:\n",
    "    if len(token) >0:\n",
    "        tokens_file.write(token+\"\\n\")\n",
    "tokens_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_tokens=pd.read_csv(\"/home/kavin/Silo/College Work/DS501/Case 5/sent_email_tokens.csv\", names=(\"tokens\",))\n",
    "recv_tokens=pd.read_csv(\"/home/kavin/Silo/College Work/DS501/Case 5/received_email_tokens.csv\", names=(\"tokens\",))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=sent_tokens[\"tokens\"].append(recv_tokens[\"tokens\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_words=pd.Series(tokens).unique()\n",
    "bag_of_words={}\n",
    "index_to_words={}\n",
    "index=0.0\n",
    "for word in unique_words:\n",
    "    bag_of_words[word]=index\n",
    "    index_to_words[index]=word\n",
    "    index+=1\n",
    "n_chars = len(tokens)\n",
    "n_vocab = len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17285"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112       To\n",
       "112    being\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[112]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "\tseq_in = tokens[i:i + seq_length][\"tokens\"]\n",
    "\tseq_out = tokens[\"tokens\"][i + seq_length]\n",
    "\tdataX.append([bag_of_words[word] for word in seq_in])\n",
    "\tdataY.append(bag_of_words[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499900"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "99840/99900 [============================>.] - ETA: 0s - loss: 7.7468Epoch 00000: loss improved from inf to 7.74687, saving model to weights-improvement-00-7.7469.hdf5\n",
      "99900/99900 [==============================] - 442s - loss: 7.7469   \n",
      "Epoch 2/5\n",
      "89344/99900 [=========================>....] - ETA: 45s - loss: 7.5464"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-13-1.7987.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(n_vocab)\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tsys.stdout.write(result)\n",
    "\tpattern.append(index)\n",
    "\tpattern = pattern[1:len(pattern)]\n",
    "print \"\\nDone.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfcpu",
   "language": "python",
   "name": "tfcpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
